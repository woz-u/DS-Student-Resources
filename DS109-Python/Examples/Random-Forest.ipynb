{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will use the Iris flower dataset from sklearn to introduce classification with Random Forest.\n",
    "\n",
    "Feature importance and partial dependency plots will be created once the model is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "1. Set up train-test data\n",
    "1. Review decision trees\n",
    "1. Visualize a decision tree\n",
    "1. Introduction to random forest\n",
    "1. Train and predict with a random forest\n",
    "1. Visualize feature importances\n",
    "1. Create partial dependency plots for random forest\n",
    "1. Knowledge check and questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "-Python imports\n",
    "\n",
    "-Train-test split\n",
    "\n",
    "-Classification metrics\n",
    "\n",
    "-Decision Trees\n",
    "\n",
    "-Measures of node impurity (Shannon Entropy and Gini Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "1. Apply a random forest classifier to a dataset\n",
    "1. Visualize feature importances from a trained random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "This notebook was made with the following packages:\n",
    "1. python=3.7.6\n",
    "1. sklearn=0.22.1\n",
    "1. matplotlib=3.1.3\n",
    "1. pandas=1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df['label'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[data.feature_names], \n",
    "    df['label'], \n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first build a decision tree model to review their structure before moving on to random forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "mdl = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt, \n",
    "                   feature_names=data.feature_names,  \n",
    "                   class_names=data.target_names,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "predicted_labels = dt.predict(X_test)\n",
    "plot_confusion_matrix(dt, X_test, y_test, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    scores = [accuracy_score, recall_score, precision_score, f1_score]\n",
    "    s_labels = ['Accuracy', 'Recall', 'Precision', 'F1']\n",
    "    for score, s_label in zip(scores, s_labels):\n",
    "        if s_label == 'Accuracy':\n",
    "            print(s_label + ': ' + str(score(y_test, y_pred)))\n",
    "        else:\n",
    "            print(s_label + ': ' + str(score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Knowledge\n",
    "1. Are decision trees deterministic?\n",
    "    - Yes they are deterministic. The best split will be found at each iterative step and will be used. \n",
    "1. How are decision trees split determined?\n",
    "    - Information gain or entropy reduction\n",
    "1. Are decision trees parametric? \n",
    "    - No, they are not parametric. Splits may differ in direction based on values.\n",
    "1. Decision trees often have high variance, why might that be?\n",
    "    - May split on wrong features or overfit to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From tree to forest\n",
    "\n",
    "1. How might multiple decision trees be leveraged to reduce variance?\n",
    "    1. Create multiple classifiers and average the results. Multiple weak learners can ofter produce a strong learner.\n",
    "1. Would multiple deterministic decision trees be useful?\n",
    "    1. Only if they were NOT deterministic.\n",
    "1. How could they not be deteministic?\n",
    "    1. Bootstrapping data and limiting features at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "-An ensemble method that combines many decision trees which have been given different subsets of the data and features to create a strong learner \n",
    "\n",
    "-Decision made on majority vote\n",
    "\n",
    "-Reduces variance and creates a non-deterministic model\n",
    "\n",
    "-Generally use a large number of bushy trees\n",
    "\n",
    "-Can get excellent performance with minimum tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf, X_test, y_test, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing trees in the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(est_num=0):\n",
    "    fn=data.feature_names\n",
    "    cn=data.target_names\n",
    "    tree.plot_tree(rf.estimators_[est_num],\n",
    "                   feature_names = fn, \n",
    "                   class_names = cn,\n",
    "                   filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(est_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(est_num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing only on the first split in the two trees above, we can see differences in the splits used to build the forest.\n",
    "\n",
    "The first one split on sepal lenth <= 5.35 and the second on petal length <=2.45. The depth of the trees also varies. This is due to the randomness induced in the trees with bootstrapping and feature selection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_import = pd.DataFrame()\n",
    "ft_import['Features'] = data.feature_names\n",
    "ft_import['Importance'] = rf.feature_importances_\n",
    "ft_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All importances sum to 1\n",
    "ft_import.Importance.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_import.plot.bar(x='Features', y='Importance', rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Dependency Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the marginal effect of a feature on predictions.\n",
    "\n",
    "Shows effect of predictions when all observations have a feature set to a particular value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "plot_partial_dependence(rf, X_train, ['petal length (cm)'], target='setosa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in data.target_names:\n",
    "    print('Target is: ', target)\n",
    "    plot_partial_dependence(rf, X_train, data.feature_names, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages of Random Forests\n",
    "\n",
    "1. Ensemble model (Wisdom of the Crowd)\n",
    "1. Good out-of-box performance\n",
    "1. Multiple trees can be trained at once "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cons of Random Forests\n",
    "1. Expensive to train\n",
    "2. Can produce very large model files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and evaluation\n",
    "\n",
    "1. Which model performed better?\n",
    "2. Which feature had the most influence on the random forest model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Check\n",
    "1. A random forest is the same as combining many decision trees?\n",
    "1. Name two ways in which random forests are made non-deterministic. \n",
    "1. Random forest classifiers are parametric?\n",
    "1. Name two visuals to help intrepret random forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Objectives \n",
    "1. Apply a random forest classifier to a dataset\n",
    "1. Visualize feature importances from a trained random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "1. Tune the random forest classifier\n",
    "    1. Number of estimators\n",
    "    1. Criterion: default is gini, can also try entropy\n",
    "    1. Max depth, min samples\n",
    "    1. Number of features\n",
    "1. Create a random forest regressor and test on sklearn Boston housing data\n",
    "    1. Compare to decision tree model\n",
    "    1. Plot feature importances\n",
    "    1. Create partial dependency plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Day Activities\n",
    "1. Code random forest from scratch\n",
    "1. Code partial dependecy plot function from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
