{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e6ddbb-52d3-4a02-9b69-100a4181e3b0",
   "metadata": {},
   "source": [
    "# DS107 Big Data : Lesson Three Companion Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd4bfc-9b15-4b80-87b4-b8b60eb56371",
   "metadata": {},
   "source": [
    "### Table of Contents <a class=\"anchor\" id=\"DS107L3_toc\"></a>\n",
    "\n",
    "* [Table of Contents](#DS107L3_toc)\n",
    "    * [Page 1 - Introduction](#DS107L3_page_1)\n",
    "    * [Page 2 - MapReduce](#DS107L3_page_2)\n",
    "    * [Page 3 - Hive Basics](#DS107L3_page_3)\n",
    "    * [Page 4 - Using Hive](#DS107L3_page_4)\n",
    "    * [Page 5 - Hive Queries](#DS107L3_page_5)\n",
    "    * [Page 6 - Sqoop](#DS107L3_page_6)\n",
    "    * [Page 7 - Setting up MySQL](#DS107L3_page_7)\n",
    "    * [Page 8 - Using Sqoop](#DS107L3_page_8)\n",
    "    * [Page 9 - Key Terms](#DS107L3_page_9)\n",
    "    * [Page 10 - Lesson 3 Hands-On](#DS107L3_page_10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a8c65-ed48-4354-a257-e01b0fd733d4",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 1 - Overview of this Module<a class=\"anchor\" id=\"DS107L3_page_1\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7136ef-5c40-4abe-8434-f3d817eccabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"480\"\n",
       "            src=\"https://player.vimeo.com/video/388136496\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x7fcf28349520>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "# Tutorial Video Name: MapReduce, Hive, and Sqoop\n",
    "VimeoVideo('388136496', width=720, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ad526-95fa-43c4-bc43-1fe807708ed3",
   "metadata": {},
   "source": [
    "The transcript for the above overview video **[is located here](https://repo.exeterlms.com/documents/V2/DataScience/Video-Transcripts/DSO107L03overview.zip)**.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this lesson, you will learn about the foundation of big data processing on Hadoop - MapReduce.  Then you'll begin utilizing one of the modern applications built on top of MapReduce: Hive.  Hive allows you to use SQL queries to interact with your big data, and you can even utilize Sqoop to integrate Hive with traditional databases like MySQL.\n",
    "\n",
    "By the end of this lesson, you should be able to complete the following tasks: \n",
    "\n",
    "* Understand the theory behind MapReduce \n",
    "* Understand the architecture of Hive\n",
    "* Utilize Hive to upload data and create HiveQL queries\n",
    "* Prepare MySQL for the use of Sqoop\n",
    "* Use Sqoop to import and export files from MySQL\n",
    "\n",
    "This lesson will culminate in a hands on in which you use Hive to find the most popular and the most highly rated anime shows.  \n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>You may want to watch this <a href=\"https://vimeo.com/456790695\"> recorded live workshop on the concepts in this lesson. </a> </p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990ac7a-5a2f-4744-af14-1b7e385ea484",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 2 - MapReduce<a class=\"anchor\" id=\"DS107L3_page_2\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612429df-e4ef-4d85-8460-5bc8914079c3",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "_MapReduce_ is what's known as a programming model that is used to process large amounts data across multiple computers in two steps: _map_ and _reduce_. The concept of MapReduce is to take a collection of data, such as the `crimes-sample.csv` file, _map_ (or filter) the desired data to produce a list of results in a key-value pair format, then those key-value pairs are sent to the _reduce_ step, which aggregates the results. The mapper helps optimize your work as well - you will basically throw away any data you aren't currently interested in, and then extract and organize the things that you do care about so that it can be aggregated.\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Fun Fact!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>MapReduce was the result of two Google researchers in 2004 who wrote a research paper titled <a href=\"https://research.google.com/archive/mapreduce.html\">\"MapReduce: Simplified Data Processing on Large Clusters.\"</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "This concept is best understood by visualizing what's happening with real data and code.\n",
    "\n",
    "---\n",
    "\n",
    "## How MapReduce Works\n",
    "\n",
    "Imagine you are handed the task of writing a Python program, using the `crimes-samples.csv` file as data, to count how many crime reports resulted in an arrest and how many did _not_ result in an arrest. Since MapReduce is a two step process, the work will be split into two parts. The _map_ step will process the \"Arrest\" column in the `crimes-samples.csv` file and produce a list as shown below, where \"Yes\" means an arrest occurred and \"No\" means there was not an arrest. The comma followed by the number one in the output below is used to count each instance and will make more sense in the reduce step.\n",
    "\n",
    "---\n",
    "\n",
    "### Example `Map` Results\n",
    "\n",
    "Below are some example map results: \n",
    "\n",
    "```text\n",
    "Yes,1\n",
    "No,1\n",
    "No,1\n",
    "Yes,1\n",
    "No,1\n",
    "No,1\n",
    "No,1\n",
    "Yes,1\n",
    "Yes,1\n",
    "No,1\n",
    "```\n",
    "\n",
    "Remember, MapReduce is primarily used when data processing will be distributed to multiple computers. Therefore, imagine multiple computers process their own different crime files and produce a list of results similar to the one above. Once the _map_ step is done, the next step is to _reduce_ or aggregate all of these entries into a simpler representation. \n",
    "\n",
    "---\n",
    "\n",
    "### Example `Reduce` Results\n",
    "\n",
    "Below are some example reduce results:\n",
    "\n",
    "```text\n",
    "Yes,4\n",
    "No,6\n",
    "```\n",
    "\n",
    "Under the hood, the values ```Yes``` and ```No``` are treated like keys in a dictionary with the value 1 being treated as the value. During the _reduce_ step, each line is processed and the values for each key are combined. So there were four entries that were a \"Yes\" and six entries that were a \"No\" that came out of the map step, which were added together in this reduce step.\n",
    "\n",
    "---\n",
    "\n",
    "## MapReduce Architecture\n",
    "\n",
    "Here are the steps that take place under the hood when you run a MapReduce program on a Hadoop cluster:\n",
    "\n",
    "* You (the client node) send in your code\n",
    "* YARN keeps track of what machines need to be utilized and copies data to HDFS or other distributed filing system as needed\n",
    "* MapReduce Application master runs under a NodeManager and keeps track of all the nodes that are working to complete the MapReduce task\n",
    "* The task takes place while talking to HDFS\n",
    "\n",
    "The system tries to ensure that your mapper gets run as close to where the data is located as possible, so that efficiency is increased.\n",
    "\n",
    "---\n",
    "\n",
    "## Languages for MapReduce\n",
    "\n",
    "While Hadoop was programmed in Java, you can utilize the concept of MapReduce through Python as well as many other languages.\n",
    "\n",
    "---\n",
    "\n",
    "## How MapReduce Handles Failure\n",
    "\n",
    "MapReduce can be quite slow, and isn't used as a standalone often anymore for that reason, but it does handle failure at a wide variety of levels pretty well - it is resilient.  Below, you will find a table of potential things that could go wrong and how MapReduce will handle the situation: \n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Issue</th>\n",
    "        <th>Solution</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Worker nodes have errors.</td>\n",
    "        <td>You can restart the worker as needed, and/or shift processing to a different node.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Application master goes down.</td>\n",
    "        <td>YARN will try to restart it.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Entire node goes down.</td>\n",
    "        <td>The resource manager will try to restart the process on another node.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Resource manager goes down.</td>\n",
    "        <td>MapReduce itself can't handle this, but you can set up high availability MapReduce and manage it through zookeeper. </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## How is MapReduce Used Today?\n",
    "\n",
    "MapReduce is the backbone of many other big data programs that sit in Hadoop, such as Hive, Pig, and Spark.  However, it is no longer used by itself, because in comparison to newer technology, MapReduce is clunky and slow.  Therefore, you will only learn the theory behind MapReduce, and not practice with it yourself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832358fd-55fa-420a-8aa2-46fbf2097986",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 3 - Hive Basics<a class=\"anchor\" id=\"DS107L3_page_3\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564e3e7-3a7f-4936-b3a3-2e0a9ab78512",
   "metadata": {},
   "source": [
    "# Hive Basics\n",
    "\n",
    "*Hive* allows you to write SQL queries and execute them on a Hadoop cluster, using a language called *HiveQL*, which is just another SQL variant, very similar to the SQL you have already learned.  It is interactive and scalable, allowing you to use it on an entire cluster of computers, no matter how many you have.  It's also pretty flexible, so you can use Hive to connect to other types of databases and write your own *user-defined functions* if you'd like.\n",
    "\n",
    "Hive does have a few limitations, however.  It does take a few minutes to execute queries, and it is limited compared to other programs you'll learn later, such as Pig and Spark.  It also does not allow you to perform CRUD operations because Hive does not connect to a real-time database.\n",
    "\n",
    "---\n",
    "\n",
    "## HiveQL vs. SQL\n",
    "\n",
    "Most things really are the same between HiveQL and SQL.  However, HiveQL uses views slightly differently than SQL. In SQL, when you create a view, it stores the data as a temporary table that you can access later.  With HiveQL, the view is just a construct, not data actually stored anywhere.  The purpose of the view in HiveQL is to enable you to break up complicated queries into smaller individual ones.\n",
    "\n",
    "---\n",
    "\n",
    "## How Hive Works\n",
    "\n",
    "Hive uses a *schema on read* format that takes unstructured data and only applies structure to it when it is being read and processed. This is done to speed up processing; it would be clunky and slow if you had to maintain a data structure (think tables) all the time. The opposition to schema on read is *schema on write*; this represents a traditional SQL table system.\n",
    "\n",
    "---\n",
    "\n",
    "## Ways to Run Hive\n",
    "\n",
    "While you can run Hive queries through the command line, the easiest way is simply to utilize the ```Hive View``` in Ambari, which will provide you with an interactive query editor window. You'll start working with Hive on earnest on the next page!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aade2-59af-4cbc-b5ed-da31e39f266d",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 4 - Using Hive<a class=\"anchor\" id=\"DS107L3_page_4\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2ed6e-98dd-43f7-8761-6ecc9a13af08",
   "metadata": {},
   "source": [
    "# Using Hive\n",
    "\n",
    "The easiest way to access Hive is through the Ambari Hive View.  To access it, go to the upper right hand corner and select ```Hive View```:\n",
    "\n",
    "![\"A window displays a web browser consists of five menu options and an icon and a button labeled maria_dev pops up six options are YARN Queue Manager, Files View, Hive View, Pig View, Storm View, and Tez View.\n",
    "  On the left has eighteen options. On the right three options are present in which first is selected, two buttons are present below and fifteen boxes captioned and present in the middle.\"](Media/hive1.png)\n",
    "\n",
    "Here's what the initial interface looks like when you click on it, once it goes through its service checks:\n",
    "\n",
    "![A window labeled Ambari Sandbox has five tabs on top. The tabs are labeled dashboard, servies, hosts, alerts, and admin. The bar also has a button for categories. The next bar has six tabs labeled hive, query, saved queries, history, UDF's and upload table. The query tab is selected.](Media/hive3.png)\n",
    "\n",
    "This main page is the ```Query Editor```.  It's where you'll type in your SQL code and get information about the query that ran, etc. To the left, you can see the databases that you are currently connected to, in the ```Database Explorer``` section, and to the right, you can see some buttons that will give you more info and tools to go along with your query.\n",
    "\n",
    "---\n",
    "\n",
    "## Load Data into Hive Using Upload Table Wizard\n",
    "\n",
    "The first thing you need to do is to load data into Hive.  This is done in the ```Upload Data``` section of the ```Hive View```:\n",
    "\n",
    "![A window labeled Ambari Sandbox has five tabs on top. The tabs are labeled dashboard, servies, hosts, alerts, and admin. The bar also has a button for categories. The next bar has six tabs labeled hive, query, saved queries, history, UDF's and upload table. The upload table option is selected. The screen displays two radio buttons labeled upload from local and upload from HDFS. A dropdown list box to choose the file type and a button labeled browse is used to select from local.](Media/hive8.png)\n",
    "\n",
    "You can load data in from either your local machine (if it's a relatively small file) or from HDFS (for larger data; assuming you have a system in place for getting large data into HDFS). In this case, you will just upload it from your local machine, so go ahead and leave the ```Upload from Local``` radio button selected, and then hit the ```Browse``` button on the right hand side, to search for the file.  You will be utilizing the two files ```books``` and ```bookids``` you have been using. Start with ```books```.  Once you drag and drop it in, you will see another view pop up:\n",
    "\n",
    "![A window labeled Ambari Sandbox has five tabs on top. The tabs are labeled dashboard, servies, hosts, alerts, and admin. The bar also has a button for categories. The next bar has six tabs labeled hive, query, saved queries, history, UDF's and upload table. The upload table option is selected. The screen displays two radio buttons labeled upload from local and upload from HDFS. Three dropdown list boxes to choose the file type, database, and stored as and a button labeled browse is used to select from local. A text field is used to indicate the table name. The table details are provided below.](Media/hive12.png)\n",
    "\n",
    "Here, you can give names to all of the columns and set the data types.  Note that because your data has headers, it all comes in as ```STRING``` data, which is not necessarily correct.  You'll need to change ```average_rating``` to ```FLOAT```, and ``` # num_pages```, ```ratings_count```, and ```text_reviews_count``` to ```INT```. You may have trouble changing this - try hitting the very left side of the arrow. \n",
    "\n",
    "<div class=\"panel panel-info\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Tip!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>You will need to change the default column names of column1, etc. to the values in the first row, so that you can follow along with the queries.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Here's what the end result should look like approximately: \n",
    "\n",
    "![A window with three tabs labeled script, history, and pig attempt two - completed. The data is provided in four columns and five rows. The column headings are labeled date, status, duration, and actions.](Media/hive13.png)\n",
    "\n",
    "Once you're satisfied, you will want to click the ```Upload Table```  button on the right side. It will kick off a lot of stuff, but you'll know your data is ready to roll once the ```Upload Progress``` bar goes away and you get a green message in the right hand corner of your screen that indicates a successful upload.  Then you can flip back into the ```Query``` section, and if you click the refresh button on right side of the ```Database Explorer```, and then look into the ```default``` database, you should see your data!\n",
    "\n",
    "![Database explorer with a dropdown list box labeled default and a box labeled search tables. The list of databases is provided below the search box.](Media/hive14.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4ad3c-2fe6-457e-8924-1410e951bebd",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 5 - Hive Queries<a class=\"anchor\" id=\"DS107L3_page_5\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc06a2-e702-47a1-942f-cbcbe31221a7",
   "metadata": {},
   "source": [
    "# Hive Queries\n",
    "\n",
    "Now it's on to the fun part of actually running some Hive queries! You can run all the normal SQL commands through here, but the one thing that is special to Hive is how the function ```create view``` gets used, so both queries you'll practice here make use of ```view```.  For the first one, you'll count up the number of ratings each book has. \n",
    "\n",
    "---\n",
    "\n",
    "## Count up the Number of Book Ratings\n",
    "\n",
    "The first thing you'll want to start with is ```create view```.  You are creating a view named ```topBooks```, and then you need to define it in the next few lines with the ```as```. Into ```topBooks``` you want to pick out a couple columns, so make use of that good 'ol ```select``` statement.  You can even count the number of book ids you have by wrapping the column ```bookid``` in the ```count``` function, and give it a name, too: ```ratingCount```.  Then make sure you tell Hive where these columns are coming from, using that trusty ```from``` statement.  You can then ```group by``` and ```order by``` to ensure things are going to make the most logical sense. ```desc``` tells Hive that you want to sort in descending order, with the largest on top.\n",
    "\n",
    "The next part of the query joins the view you just created together with the book titles, so that you can get a list of book titles, and not just IDs, since that wouldn't be very practical. Although it's not necessary, to keep things simple you can give each table an alias. ```bookid``` becomes ```b``` and ```topBooks``` view becomes ```t```, which allows you to easily and quickly specify what you're joining and from whence it comes.\n",
    "\n",
    "```sql\n",
    "create view topBooks as\n",
    "select bookid, count(bookid) as ratingCount\n",
    "from books\n",
    "group by bookid\n",
    "order by ratingCount desc;\n",
    "\n",
    "select b.title, ratingCount\n",
    "from topBooks t join bookids b on t.bookid = b.bookid;\n",
    "```\n",
    "\n",
    "And here are the results you receive from this query: \n",
    "\n",
    "![A bar labeled 100-percent. The screen displays the query process results with the status as succeeded. The page has two tabs labeled logs and results. The option results is chosen. It displays the title and the corresponding ratingcount.](Media/hive4.png)\n",
    "\n",
    "Results will appear on the bottom of your screen, under the ```Results``` tab.  If you need to know anything about the query you just processed, you can also check the ```Logs``` tab. Looks like in this case that all the book ids are unique (only used once), so you are getting a count of 1 for all.  But, it would be more helpful in an instance when an id wasn't unique - maybe a situation where you had a whole bunch of users rating the same book.\n",
    "\n",
    "It's important that you remove the view when you're done, since you can only have one instance. To drop the view, simply use this code: \n",
    "\n",
    "```sql\n",
    "drop view topBooks\n",
    "```\n",
    "\n",
    "If you try and run the query a second time, or even make some sort of changes and do it again, you will get an error.  It flashes by really quickly, but don't worry - you can always go back and read it in more depth in the notifications section, which is denoted by the little envelope icon on the right menu at the bottom. \n",
    "\n",
    "In case you encounter this issue, here is the error you will get:\n",
    "\n",
    "![An error message that reads, java.sql.SQLException: Error while processing statement. FAILED: Execution error, return code 1 from org apache hadoop hive.sql.exec.DDLTask.AlreadyExistsException open bracket message: Table topBooks already exists close bracket.](Media/hive2.png)\n",
    "\n",
    "The ```AlreadyExistsException``` is the giveaway that tells you this error is about a repeat view, and it will go on to tell you exactly which view you already have.\n",
    "\n",
    "---\n",
    "\n",
    "## Saving your Queries\n",
    "\n",
    "You can save your Hive queries by clicking on the ```Save as``` button at the bottom, which will pop up this window and enable to you to give the query a name and save: \n",
    "\n",
    "![A window labeled Ambari Sandbox has five tabs on top. The tabs are labeled dashboard, servies, hosts, alerts, and admin. The bar also has a button for categories. The next bar has six tabs labeled hive, query, saved queries, history, UDF's and upload table. The query tab is selected. A prompt box labeled saving item has a dropdown list box and two buttons labeled close and ok.](Media/hive5.png)\n",
    "\n",
    "It can then be accessed on the ```Saved Queries``` tab along the top:\n",
    "\n",
    "![A window labeled Ambari Sandbox has five tabs on top. The tabs are labeled dashboard, servies, hosts, alerts, and admin. The bar also has a button for categories. The next bar has six tabs labeled hive, query, saved queries, history, UDF's and upload table. The saved queries tab is selected. The screen displays four fields labeled preview, title, database, and owner. A button labeled clear filters is placed at the end of the fields.](Media/hive6.png)\n",
    "\n",
    "You can access a new query window by clicking on the ```New Worksheet``` button on the bottom while on the ```Query``` tab.  Go ahead and do this now, as you'll attempt a new query to find the highest rated book. \n",
    "\n",
    "---\n",
    "\n",
    "## Find the Highest Rated Book\n",
    "\n",
    "How about trying to find the highest rated book(s)? You can keep all the count info in, but you will also ```select``` the ```average_rating``` column and use the function ```avg``` on it.  Since you learned above that the books are only listed once, performing an aggregate function is a little redundant, but it's good to know in case you face a situation in which you have non-unique IDs, so you'll just roll with it for now. \n",
    "\n",
    "Then you can order by the ```ratingAvg``` instead of by ```ratingCount```, and perform the same ```join``` as before.  You'll be able to see which books had the highest ratings!\n",
    "\n",
    "```sql\n",
    "create view topBooks2 as \n",
    "select bookid, avg(average_rating) as ratingAvg, count(bookid) as ratingCount\n",
    "from books\n",
    "group by bookid\n",
    "order by ratingAvg desc;\n",
    "\n",
    "select b.title, ratingAvg\n",
    "from topBooks2 t join bookids b on t.bookid = b.bookid;\n",
    "```\n",
    "\n",
    "And here are the results of that query:\n",
    "\n",
    "![A screen displays the query process results with the status as succeeded. The page has two tabs labeled logs and results. The option results is chosen. It displays the title and the corresponding ratingavg.](Media/hive7.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Is Your Query Done?\n",
    "\n",
    "Sometimes things can take a while to run, and sometimes you may be left wondering if your computer is finished processing your Hive Query.  If you ever wonder whether something ran or not, you can check in several places: \n",
    "\n",
    "* In the Query tab, if the button at the bottom is green and says ```Execute```, then something has finished running, whether you have results or you got an error.\n",
    "* In the notifications section of the Query tab, you will see a message that will state whether an error occurred.\n",
    "* In the History tab, you can see what was processed and run successfully or errored out.  Don't worry, you can see how many times even your instructor strikes out here! Learning is a trial-and-error process, so that comes with a lot of errors before you hit success! \n",
    "\n",
    "    ![A window labeled Ambari Sandbox has five tabs on top. The tabs are labeled dashboard, servies, hosts, alerts, and admin. The bar also has a button for categories. The next bar has six tabs labeled hive, query, saved queries, history, UDF's and upload table. The history tab is selected. It has five fields labeled title, status, 10/03/2019, 10/08/2019, and time scale. It also displays two buttons for refresh and clear filters.](Media/hive15.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Have you Forgotten to Drop Views?\n",
    "\n",
    "Views take up a lot of space, and because you're only running one node on a virtual machine, you are probably a little short on space.  If you get anything like the error below, remember to drop all your views and try again.  Chances are, you've just run out of working memory!\n",
    "\n",
    "![An error message that reads, java.sql.SQLException: Error while processing statement. FAILED: Execution error, return code 2 from org apache hadoop hive.ql.exec.tez.TezTask.](Media/hive16.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e3b8a-9224-447f-8cf7-5035cae86421",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 6 - Sqoop<a class=\"anchor\" id=\"DS107L3_page_6\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e2098-28ef-4ff5-a155-afe824de4323",
   "metadata": {},
   "source": [
    "# Sqoop\n",
    "\n",
    "*Sqoop* is a program that allows you to use Hadoop to run queries on databases that are stored in MySQL, allowing you to integrate the processing power of Hadoop with your already created database systems. It kicks off MapReduce but just uses mappers to talk with HDFS and doesn't utilize reducers.  Then you can use Hive or Pig on it if you'd like. Sqoop does not have an Ambari interface, so in order to make use of this connector, you'll need to work with the command line.\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Fun Fact!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>The name Sqoop came about as SQL and Hadoop squished together.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb7d8a-aced-42e5-ac24-32acf8c95473",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 7 - Setting up MySQL<a class=\"anchor\" id=\"DS107L3_page_7\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0debaf-5fae-4035-8e7d-ae62e62456cb",
   "metadata": {},
   "source": [
    "# Setting up MySQL\n",
    "\n",
    "Your goal is to learn how to use Sqoop to \"scoop out\" data from MySQL and process it on your Hadoop cluster. However, you'll first need to set up MySQL for this.  Amongst other things, you'll create a data table in MySQL.  In real life, you'd already have data to connect to, but since you're just practicing, you don't. \n",
    "\n",
    "---\n",
    "\n",
    "## Login to MySQL\n",
    "\n",
    "Luckily, Hortonworks already comes with MySQL, so you don't need to install it! Just log in to MySQL, using the ```mysql``` command and specifying that you are going to login as a ```root``` user, and use ```-p``` to ask for a password prompt: \n",
    "\n",
    "```bash\n",
    "mysql -u root -p\n",
    "```\n",
    "\n",
    "Just like other things, the default password for Hortonworks MySQL is ```hadoop```, so type that in when prompted.  Remember that you might not be able to see what you type, but go for it anyway and hit enter.\n",
    "\n",
    "You will know you are into MySQL when you get the ```mysql>``` prompt in the command line.\n",
    "\n",
    "---\n",
    "\n",
    "## Create a Database\n",
    "\n",
    "Now you can go ahead and create your database, using the simple command ```create database```!\n",
    "\n",
    "```sql\n",
    "create database books;\n",
    "```\n",
    "\n",
    "Where ```books``` is the name of the database you are going to be creating. You should receive a prompt like this one when complete: \n",
    "\n",
    "```text\n",
    "Query OK, 1 row affected (0.00 sec)\n",
    "```\n",
    "\n",
    "To double check that it worked, you can also use the command ```show databases;``` which will provide you a list of all databases in MySQL:\n",
    "\n",
    "```sql\n",
    "show databases;\n",
    "```\n",
    "\n",
    "This should be your result:\n",
    "\n",
    "```text\n",
    "+--------------------+\n",
    "| Database           |\n",
    "+--------------------+\n",
    "| information_schema |\n",
    "| books              |\n",
    "| hive               |\n",
    "| mysql              |\n",
    "| performance_schema |\n",
    "| ranger             |\n",
    "+--------------------+\n",
    "6 rows in set (0.01 sec)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Getting Data into MySQL\n",
    "\n",
    "You can then run this stuff line by line to create a few entries for the ```books1``` table. First tell SQL to use the `books` table: \n",
    "\n",
    "```sql\n",
    "USE books;\n",
    "```\n",
    "\n",
    "Then tell SQL to ```Begin;```:\n",
    "\n",
    "```sql\n",
    "BEGIN;\n",
    "```\n",
    "\n",
    "Then ensure that you drop the table if you already have one:\n",
    "\n",
    "```sql\n",
    "DROP TABLE IF EXISTS books1;\n",
    "```\n",
    "\n",
    "Then here comes your ```create table``` command:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE books1 (\n",
    "  id integer NOT NULL,\n",
    "  authors varchar(255),\n",
    "  average_rating integer NOT NULL,\n",
    "  language_code varchar(255),\n",
    "  num_pages integer NOT NULL,\n",
    "  ratings_count integer NOT NULL,\n",
    "  text_reviews_count integer NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "And then you will enter in some data into that table, making use of ```insert into```.  Note that this is not the entirety of the dataset; just the first five rows and a few of the columns to practice with.  It would be pretty time consuming to manually all of this data this way, and you are assuming that in the real world your MySQL database would be connected to data entered by a user of some sort.\n",
    "\n",
    "```sql\n",
    "INSERT INTO books1 VALUES (1,'J.K. Rowling', 4.56, 'eng', 652, 1944099, 26249),(2,'J.K.Rowling', 4.49, 'eng', 870, 1996446,27613),(3,'J.K. Rowling', 4.47, 'eng', 320,5629932,70390),(4,'J.K. Rowling', 4.41, 'eng', 352, 6267,272),(5,'J.K. Rowling', 4.55, 'eng', 435,2149872,33964);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Make Sure it can Use UTF8\n",
    "\n",
    "Now there are just a few little formatting things to add to the settings for MySQL.  You will set both ```names``` and ```character``` to the ```utf8``` format.\n",
    "\n",
    "```sql\n",
    "set names 'utf8';\n",
    "```\n",
    "\n",
    "```sql\n",
    "set character set utf8;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Examine the Data You Brought in\n",
    "\n",
    "Now that you've inserted some data into your table, take a look-see to ensure that it came in as expected.  Go ahead and tell MySQL to look at the ```books``` database:\n",
    "\n",
    "```sql\n",
    "use books;\n",
    "```\n",
    "\n",
    "Then ask it to show all the tables:\n",
    "\n",
    "```sql\n",
    "show tables;\n",
    "```\n",
    "\n",
    "And lastly, ```select``` everything from the table ```books1```:\n",
    "\n",
    "```sql\n",
    "select * from books1;\n",
    "```\n",
    "\n",
    "It should all be there and ready to roll!\n",
    "\n",
    "---\n",
    "\n",
    "## Set Privileges \n",
    "\n",
    "The last thing you will need to do in MySQL is to set privileges so that you to run Sqoop on your local host and connect to MysQL:\n",
    "\n",
    "```sql\n",
    "grant all privileges on books.* to ''@'localhost';\n",
    "```\n",
    "\n",
    "Once that's done, you'll exit out of MySQL.\n",
    "\n",
    "---\n",
    "\n",
    "## Exiting MySQL\n",
    "\n",
    "If you want to exit out of MySQL, it's as simple as saying so: \n",
    "\n",
    "```bash\n",
    "exit\n",
    "```\n",
    "\n",
    "MySQL politely tells you \"Bye!\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3f800-0a14-4d13-8c32-390965c537ca",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 8 - Using Sqoop<a class=\"anchor\" id=\"DS107L3_page_8\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597e4b4-ce22-448d-843e-eab5ba74a9c4",
   "metadata": {},
   "source": [
    "# Using Sqoop\n",
    "\n",
    "Now you have MySQL all set up, it's time to finally utilize Sqoop!\n",
    "\n",
    "---\n",
    "\n",
    "## Suck out Data from MySQL to HDFS using Sqoop\n",
    "\n",
    "There are two options for where to put the data from MySQL: it can go directly into HDFS (Files View), or it can go into Hive.  You'll start by putting data into HDFS.\n",
    "\n",
    "The line below uses the ```sqoop``` command to connect to MySQL.  ```--driver``` helps make things run smoothly, and the command ```--table``` is for creating a new table.  Whatever comes after ```--table``` is the name of your new table:\n",
    "\n",
    "```bash\n",
    "sqoop import --connect jdbc:mysql://localhost/books --driver com.mysql.jdbc.Driver --table books1 -m 1\n",
    "```\n",
    "\n",
    "Lots goes on under the hood, including kicking off MapReduce jobs to get the data in. You can confirm by going into your HDFS view that the file has been created.  It will also provide you will a ```_SUCCESS``` file.\n",
    "\n",
    "![A window labeled Ambari Sandbox has five tabs on top. The tabs are labeled dashboard, servies, hosts, alerts, and admin. The bar also has a button for categories. The screen displays the data in name, size, last modified date, owner, group, and permission. It has three buttons labeled select all, new folder, and upload on top.](Media/hive9.png)\n",
    "\n",
    "Then go to `users` and `maria_dev` and click to open the file, and you should see this:\n",
    "\n",
    "![A window for file preview. The location is given as /user/maria_dev/books1/part-m-00000. It displays five data. Two buttons labeled cancel and download are placed at the bottom of the page.](Media/hive10.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Suck out Data from MysQL to Hive using Sqoop\n",
    "\n",
    "You can also put data from MySQL directly into Hive. It's simple - all you need to add is ```--hive-import``` to the end, and Hadoop will take care of it!\n",
    "\n",
    "```bash\n",
    "sqoop import --connect jdbc:mysql://localhost/books --driver com.mysql.jdbc.Driver --table books1 -m 1 --hive-import\n",
    "```\n",
    "\n",
    "This may take a while, and it may be stuck on a particular part, but don't panic! It should move on ok.\n",
    "\n",
    "Then you can check to see if its there, by going to the ```Hive view``` and look under the ```default``` database and see your table. \n",
    "\n",
    "![A window labeled Ambari Sandbox has five tabs on top. The tabs are labeled dashboard, servies, hosts, alerts, and admin. The bar also has a button for categories. The next bar has six tabs labeled hive, query, saved queries, history, UDF's and upload table. The query tab is selected.](Media/hive11.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Put Data Back into MySQL from Hive\n",
    "\n",
    "You can also go the other way - take data that you've used in Hive and put it into a MySQL database.  \n",
    "\n",
    "Data used in Hive lives in the ```Files view```, under the ```App``` directory, under ```hive``` and then in the ```warehouse``` directory. You should see that there is a ```books``` file there. That file is what you'll be transferring back.  \n",
    "\n",
    "---\n",
    "\n",
    "### Create a New Table in MySQL\n",
    "\n",
    "But before you do anything else, however, you need to create a table  that will contain your data. Start by signing into MySQL again:\n",
    "\n",
    "```bash\n",
    "mysql -u root -p\n",
    "```\n",
    "\n",
    "Your password will, of course, be ```hadoop```.  Then you can use the ```books``` table you created:\n",
    "\n",
    "```sql\n",
    "use books;\n",
    "```\n",
    "\n",
    "And then finally create a table:\n",
    "\n",
    "```sql\n",
    "create table exported_books3 (bookid INTEGER, authors VARCHAR(255), average_rating INTEGER, language_code VARCHAR(255), num_pages INTEGER, ratings_count INTEGER, text_reviews_count INTEGER);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Export Data Using Sqoop\n",
    "\n",
    "And now exit out of MySQL, and then you can export: \n",
    "\n",
    "```bash\n",
    "sqoop export --connect jdbc:mysql://localhost/books -m 1 --driver com.mysql.jdbc.Driver --table exported_books3 --export-dir /apps/hive/warehouse/books1 --input-fields-terminated-by '\\0001'\n",
    "```\n",
    "\n",
    "Now, to check to see if it worked, you will want to log back into MySQL, then change to the ```books``` database:\n",
    "\n",
    "```sql\n",
    "use books;\n",
    "```\n",
    "\n",
    "Then you can look at everything in the ```exported_books3``` table:\n",
    "\n",
    "```sql\n",
    "select * from exported_books3\n",
    "```\n",
    "\n",
    "And if its worked properly, what was now empty should be populated with the data! Just like below!\n",
    "\n",
    "```text\n",
    "+--------+--------------+----------------+---------------+-----------+---------------+--------------------+\n",
    "| bookid | authors      | average_rating | language_code | num_pages | ratings_count | text_reviews_count |\n",
    "+--------+--------------+----------------+---------------+-----------+---------------+--------------------+\n",
    "|      1 | J.K. Rowling |              5 | eng           |       652 |       1944099 |              26249 |\n",
    "|      2 | J.K.Rowling  |              4 | eng           |       870 |       1996446 |              27613 |\n",
    "|      3 | J.K. Rowling |              4 | eng           |       320 |       5629932 |              70390 |\n",
    "|      4 | J.K. Rowling |              4 | eng           |       352 |          6267 |                272 |\n",
    "|      5 | J.K. Rowling |              5 | eng           |       435 |       2149872 |              33964 |\n",
    "+--------+--------------+----------------+---------------+-----------+---------------+--------------------+\n",
    "5 rows in set (0.00 sec)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36943bcd-7e16-4af1-97d3-c09e81ae5e99",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 9 - Key Terms<a class=\"anchor\" id=\"DS107L3_page_9\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2cdde-9447-49b5-8936-17d80f46b45e",
   "metadata": {},
   "source": [
    "# Key Terms\n",
    "\n",
    "Below is a list and short description of the important keywords learned in this lesson. Please read through and go back and review any concepts you do not fully understand. Great Work!\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>MapReduce</td>\n",
    "        <td>A programming model used to process big data in parallel using a map procedure to filter and process, and a reduce procedure to perform data aggregation and summarization.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Hive</td>\n",
    "        <td>A program that allows you to write SQL queries for your Hadoop cluster.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>HiveQL</td>\n",
    "        <td>The Hive brand of SQL.  Primarily only differs in how views are used.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>User-Defined Functions</td>\n",
    "        <td>Functions you, the user, create.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Schema on Read</td>\n",
    "        <td>Storing unstructured data and only giving the data a structure when you go to use it.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Schema on Write</td>\n",
    "        <td>Storing data in structured tables.  A traditional SQL storage system.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Sqoop</td>\n",
    "        <td>A program to integrate Hive and traditional database connections like MySQL.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key SQL Commands\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>create view</td>\n",
    "        <td>Makes a view</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>drop view</td>\n",
    "        <td>Removes a view.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>create database</td>\n",
    "        <td>Sets up an empty database.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>show database</td>\n",
    "        <td>Allows you to view all the tables in your database.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>drop table if exists</td>\n",
    "        <td>Allows you to overwrite a table.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>use</td>\n",
    "        <td>Sets the table you're working in.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key Command Line Code\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>mysql -u root -p</td>\n",
    "        <td>Connection sequence to get into MySQL.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>sqoop import</td>\n",
    "        <td>Imports data from a database connection.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>--hive-import</td>\n",
    "        <td>A specifier to sqoop import that allows you to import data from a database connection directly into Hive.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>sqoop export</td>\n",
    "        <td>Exports data from Hadoop into a database connection.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d02d5-04fa-4220-9637-d02c3b32dce5",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 10 - Lesson 3 Hands-On<a class=\"anchor\" id=\"DS107L3_page_10\"></a>\n",
    "\n",
    "[Back to Top](#DS107L3_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3bf3e-ed7b-44cf-b7b3-e96eac0a2640",
   "metadata": {},
   "source": [
    "In this lesson, you've learned Hive and Sqoop. Now it's time to practice with a Hands-On project! This Hands-Â­On **will** be graded, so make sure you complete each part. \n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Do not submit your project until you have completed all requirements, as you will not be able to resubmit.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "**[Here](https://repo.exeterlms.com/documents/V2/DataScience/Big-Data/rating.zip)** is a dataset on ratings for Anime, and a **[corresponding dataset](https://repo.exeterlms.com/documents/V2/DataScience/Big-Data/anime.zip)** that has their titles and other information about them.  Please upload these data files into Hive and then determine the following:\n",
    "\n",
    "* Determine which Anime show has been rated the most times.\n",
    "* Find out which Anime show is the highest rated.\n",
    "* Do the highest anime ratings differ for Anime shows that have been rated by more than ten people? \n",
    "* Find out which Anime show is the highest rated among only those shows in the \"Slice of Life\" genre. \n",
    "\n",
    "Then, practice using Sqoop by creating a 5-row table of your own design in MySQL and importing it directly into HDFS and Hive.  Then, export the data back from Hive into a new MySQL table.  Take screenshots along the way to demonstrate this has been done.\n",
    "\n",
    "When you have completed the assignment, please include a document with your HiveQL queries, screenshots of your results, and the process of using Sqoop.\n",
    "\n",
    "---\n",
    "\n",
    "## Alternative Assignment if You Can't Run Hadoop and/or Ambari\n",
    "\n",
    "If your computer refuses to run Hadoop and/or Ambari, **[here](https://repo.exeterlms.com/documents/V2/DataScience/Big-Data/L3exam.zip)** is an alternative exam to test your understanding of the material. Please attach it instead.\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Be sure to zip and submit your entire directory when finished!</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
