{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7d97c47c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Ch10-deeplearning-lab\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd8910",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfb8d92a",
   "metadata": {},
   "source": [
    "# Lab: Deep Learning\n",
    "\n",
    "In this section, we show how to fit the examples discussed in the text. We use the `luz` package, which interfaces to the\n",
    "`torch` package which in turn links to efficient\n",
    "`C++` code in the LibTorch library. \n",
    "\n",
    "This version of the lab was produced by Daniel Falbel and Sigrid\n",
    "Keydana, both data scientists at **Rstudio** where these packages were\n",
    "produced.\n",
    "\n",
    "An advantage over our original `keras` implementation is that this\n",
    "version does not require a separate `python` installation.\n",
    "\n",
    "## Single Layer Network on Hitters Data\n",
    "\n",
    "We start by fitting the models in Section 10.6.\n",
    "We set up the data, and separate out a training and\n",
    "test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7324ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ISLR2)\n",
    "Gitters <- na.omit(Hitters)\n",
    "n <- nrow(Gitters)\n",
    "set.seed(13)\n",
    "ntest <- trunc(n / 3)\n",
    "testid <- sample(1:n, ntest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03925331",
   "metadata": {},
   "source": [
    "The linear model should be familiar, but we present it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfit <- lm(Salary ~ ., data = Gitters[-testid, ])\n",
    "lpred <- predict(lfit, Gitters[testid, ])\n",
    "with(Gitters[testid, ], mean(abs(lpred - Salary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5846da",
   "metadata": {},
   "source": [
    "Notice the use of the `with()` command: the first argument is a\n",
    "dataframe, and the second an expression that can refer to elements of\n",
    "the dataframe by name. In this instance the dataframe corresponds to\n",
    "the test data and the expression computes the mean absolute prediction\n",
    "error on this data.\n",
    "\n",
    "Next we fit the lasso using `glmnet`. Since this package does\n",
    "not use formulas, we create `x` and `y` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20becd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- scale(model.matrix(Salary ~ . - 1, data = Gitters))\n",
    "y <- Gitters$Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32235569",
   "metadata": {},
   "source": [
    "The first line makes a call\n",
    "to `model.matrix()`,  which produces the same matrix that was\n",
    "used by `lm()` (the `-1`  omits the intercept).\n",
    "This function automatically converts factors to dummy variables.\n",
    "The `scale()` function standardizes the matrix so each column\n",
    "has mean zero and variance one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(glmnet)\n",
    "cvfit <- cv.glmnet(x[-testid, ], y[-testid],\n",
    "    type.measure = \"mae\")\n",
    "cpred <- predict(cvfit, x[testid, ], s = \"lambda.min\")\n",
    "mean(abs(y[testid] - cpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b30be",
   "metadata": {},
   "source": [
    "To fit the neural network, we first set up a model structure\n",
    "that describes the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(torch)\n",
    "library(luz) # high-level interface for torch\n",
    "library(torchvision) # for datasets and image transformation\n",
    "library(torchdatasets) # for datasets we are going to use\n",
    "library(zeallot)\n",
    "torch_manual_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "modnn <- nn_module(\n",
    "  initialize = function(input_size) {\n",
    "    self$hidden <- nn_linear(input_size, 50)\n",
    "    self$activation <- nn_relu()\n",
    "    self$dropout <- nn_dropout(0.4)\n",
    "    self$output <- nn_linear(50, 1)\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    x %>% \n",
    "      self$hidden() %>% \n",
    "      self$activation() %>% \n",
    "      self$dropout() %>% \n",
    "      self$output()\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134e695",
   "metadata": {},
   "source": [
    "We have created a model called `modnn` by defining the `initialize()` and `forward()` functions and passing them to the `nn_module()` function. The `initialize()` function is responsible for initializing the submodules that are used by the model. In the `forward` method we implement what happens when the model is called on input data. In this case we use the layers we defined in `initialize()` in that specific order.\n",
    "\n",
    "`self` is a list-like special object that is used to share information between the methods of the `nn_module()`. When you assign an object to `self` in `initialize()`, it can then be accessed by `forward()`.\n",
    "\n",
    "The `pipe` operator `%>%`\n",
    " passes the previous term as the first argument to the next\n",
    "function, and returns the result.\n",
    "\n",
    "We illustrate the use of the pipe operator on a simple example. Earlier, we created `x` using the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a73807",
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- scale(model.matrix(Salary ~ . - 1, data = Gitters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210c1bc",
   "metadata": {},
   "source": [
    "We first make a matrix, and then we center and scale each of the variables.\n",
    "Compound expressions like this can be difficult to parse. We could have obtained the same result using the pipe operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- model.matrix(Salary ~ . - 1, data = Gitters) %>% scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64aa7a",
   "metadata": {},
   "source": [
    "Using the pipe operator makes it easier to follow the sequence of operations.\n",
    "\n",
    "We now return to our neural network. The object `modnn` has a single hidden layer with 50 hidden units, and\n",
    "a ReLU activation function. It then has a dropout layer, in which a\n",
    "random 40% of the 50 activations from the previous layer are set to zero\n",
    "during each iteration of the stochastic gradient descent\n",
    "algorithm. Finally, the output layer has just one unit with no\n",
    "activation function, indicating that the model provides a single\n",
    "quantitative output.\n",
    "\n",
    "Next we add details to  `modnn` that control the fitting\n",
    "algorithm. We minimize squared-error loss as in\n",
    "(10.22). The algorithm\n",
    "tracks the mean absolute error on the training data, and\n",
    "on validation data if it is supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modnn <- modnn %>% \n",
    "  setup(\n",
    "    loss = nn_mse_loss(),\n",
    "    optimizer = optim_rmsprop,\n",
    "    metrics = list(luz_metric_mae())\n",
    "  ) %>% \n",
    "  set_hparams(input_size = ncol(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a042df1",
   "metadata": {},
   "source": [
    "In the previous line, the pipe operator passes `modnn` as the first argument to `setup()`. \n",
    "The `setup()` function embeds these specification into a new model object. \n",
    "We also use `set_hparam()` to specify the arguments that should be passed to the \n",
    "`initialize()` method of `modnn`. \n",
    "\n",
    "Now we fit the model. We supply the training data and the number of `epochs`. By default,\n",
    "at each step of SGD, the algorithm randomly selects 32 training observations for \n",
    "the computation of the gradient. Recall from Sections 10.4 and 10.7\n",
    "that an epoch amounts to the number of SGD steps required to process $n$\n",
    "observations. Since the training set has\n",
    "$n=176$, an epoch is $176/32=5.5$ SGD steps. The `fit()` function has an argument\n",
    "`valid_data`; these data are not used in the fitting,\n",
    "but can be used to track the progress of the model (in this case reporting\n",
    "mean absolute error). Here we\n",
    "actually supply the test data so we can see mean absolute error of both the\n",
    "training data and test data as the epochs proceed. To see more options\n",
    "for fitting, use `?fit.luz_module_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209afae0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fitted <- modnn %>% \n",
    "  fit(\n",
    "    data = list(x[-testid, ], matrix(y[-testid], ncol = 1)),\n",
    "    valid_data = list(x[testid, ], matrix(y[testid], ncol = 1)),\n",
    "    epochs = 20 # 50\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e2b1b",
   "metadata": {},
   "source": [
    "*(Here and elsewhere we have reduced the number of epochs to make\n",
    "    runtimes manageable; users can of course change back)*\n",
    "\n",
    "We can plot the `fitted` model to display the mean absolute error for the training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24588d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed897465",
   "metadata": {},
   "source": [
    "Finally, we predict from the final model, and\n",
    "evaluate its performance on the test data. Due to the use of SGD, the results vary slightly with each\n",
    "fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4c967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79871c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "npred <- predict(fitted, x[testid, ])\n",
    "mean(abs(y[testid] - as.matrix(npred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c31846",
   "metadata": {},
   "source": [
    "We had to convert the `npred` object to a matrix, since the current\n",
    "predict method returns an object of class `torch_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class(npred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb95de3",
   "metadata": {},
   "source": [
    "## Multilayer Network on the MNIST Digit Data\n",
    "\n",
    "The `torchvision` package comes with a number of example datasets,\n",
    "including the `MNIST` digit data. Our first step is to load the\n",
    "`MNIST` data. The `mnist_dataset()` function is provided for this purpose.\n",
    "\n",
    "This functions returns a `dataset()`, a data structure implemented in `torch`\n",
    "allowing one to represent any dataset without making assumptions on where the data is stored and how the data is organized. Usually, torch datasets also implement the \n",
    "data acquisition process, like downloading and caching some files on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac31e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds <- mnist_dataset(root = \".\", train = TRUE, download = TRUE)\n",
    "test_ds <- mnist_dataset(root = \".\", train = FALSE, download = TRUE)\n",
    "\n",
    "str(train_ds[1])\n",
    "str(test_ds[2])\n",
    "\n",
    "length(train_ds)\n",
    "length(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0a0df",
   "metadata": {},
   "source": [
    "There are 60,000 images in the training data and 10,000 in the test data. The images are $28\\times 28$, and stored as matrix of pixels. We need to transform each one into a vector.  \n",
    "\n",
    "Neural networks are somewhat sensitive to the scale of the inputs. For example, ridge and\n",
    "lasso regularization are affected by scaling.  Here the inputs are eight-bit\n",
    "grayscale values between 0 and 255, so we rescale to the unit\n",
    "interval. (Note: eight bits means $2^8$, which equals 256. Since the convention\n",
    "is to start at $0$, the possible values  range from $0$ to $255$.)\n",
    "\n",
    "To apply these transformations we will re-define `train_ds` and `test_ds`, now passing a the `transform` argument that will apply a transformation to each of\n",
    "the image inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa08fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform <- function(x) {\n",
    "  x %>% \n",
    "    torch_tensor() %>% \n",
    "    torch_flatten() %>% \n",
    "    torch_div(255)\n",
    "}\n",
    "train_ds <- mnist_dataset(\n",
    "  root = \".\", \n",
    "  train = TRUE, \n",
    "  download = TRUE, \n",
    "  transform = transform\n",
    ")\n",
    "test_ds <- mnist_dataset(\n",
    "  root = \".\", \n",
    "  train = FALSE, \n",
    "  download = TRUE,\n",
    "  transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab56131",
   "metadata": {},
   "source": [
    "Now we are ready to fit our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676db8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnn <- nn_module(\n",
    "  initialize = function() {\n",
    "    self$linear1 <- nn_linear(in_features = 28*28, out_features = 256)\n",
    "    self$linear2 <- nn_linear(in_features = 256, out_features = 128)\n",
    "    self$linear3 <- nn_linear(in_features = 128, out_features = 10)\n",
    "    \n",
    "    self$drop1 <- nn_dropout(p = 0.4)\n",
    "    self$drop2 <- nn_dropout(p = 0.3)\n",
    "    \n",
    "    self$activation <- nn_relu()\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    x %>% \n",
    "      \n",
    "      self$linear1() %>% \n",
    "      self$activation() %>% \n",
    "      self$drop1() %>% \n",
    "      \n",
    "      self$linear2() %>% \n",
    "      self$activation() %>% \n",
    "      self$drop2() %>% \n",
    "      \n",
    "      self$linear3()\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce838a5",
   "metadata": {},
   "source": [
    "We define the `intialize()` and `forward()` methods of the `nn_module()`.\n",
    "\n",
    "In `initialize` we specify all layers that are used in the model.\n",
    "For example, `nn_linear(784, 256)` defines a dense layer that goes from\n",
    "$28\\times28=784$ input units to a hidden layer of $256$ units. The model\n",
    "will have 3 of them, each one decreasing the number of output units. The last\n",
    "will have 10 output units, because each unit will be associated to a different\n",
    "class, and we have a 10-class classification problem.\n",
    "We also defined dropout layers using `nn_dropout()`. These will be used\n",
    "to perform dropout regularization. Finally we define the activation\n",
    "layer using `nn_relu()`.\n",
    "\n",
    "In `forward()` we define the order in which these layers are called. We call them in blocks like (linear, activation, dropout), except for the last layer that does not\n",
    "use an activation function or dropout.\n",
    "\n",
    "Finally, we use `print` to summarize the model, and to make sure we got it\n",
    "all right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelnn())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e20a5",
   "metadata": {},
   "source": [
    "The parameters for each layer include a bias term, which results in a\n",
    "parameter count of 235,146. For example, the first hidden\n",
    "layer involves $(784+1)\\times 256=200{,}960$ parameters.\n",
    "\n",
    "Next, we add details to the model to specify the fitting algorithm. We fit the model by minimizing the cross-entropy function given by (10.13).\n",
    "\n",
    "Notice that in `torch` the cross entropy function is defined in terms of \n",
    "the logits, for numerical stability and memory efficiency reasons. It does not require the target to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa46725",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnn <- modelnn %>% \n",
    "  setup(\n",
    "    loss = nn_cross_entropy_loss(),\n",
    "    optimizer = optim_rmsprop, \n",
    "    metrics = list(luz_metric_accuracy())\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd9454",
   "metadata": {},
   "source": [
    "Now we are ready to go. The final step is to supply training data, and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f7996",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.time(\n",
    "   fitted <- modelnn %>%\n",
    "      fit(\n",
    "        data = train_ds, \n",
    "        epochs = 10, #15, \n",
    "        valid_data = 0.2,\n",
    "        dataloader_options = list(batch_size = 256),\n",
    "        verbose = TRUE\n",
    "      )\n",
    " )\n",
    "plot(fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a7486",
   "metadata": {},
   "source": [
    "We have suppressed the output here. The output is a progress report on the\n",
    "fitting of the model, grouped by epoch. This is very useful, since on\n",
    "large datasets fitting can take time. Fitting this model took 215\n",
    "seconds on a 2.7GHz MacBook Pro with 4 cores and 16 GB of RAM.\n",
    "Here we specified a\n",
    "validation split of 20%, so training is actually performed on\n",
    "80% of the 60,000 observations in the training set. This is an\n",
    "alternative to actually supplying validation data, like we did in\n",
    "Section 10.9.1. See\n",
    "`?fit.luz_module_generator` for all the optional fitting arguments. SGD  uses batches\n",
    "of 256 observations in computing the gradient, and doing the\n",
    "arithmetic, we see that an epoch corresponds to 188 gradient steps.\n",
    "The last `plot()` command produces a figure similar to Figure 10.18.\n",
    "\n",
    "To obtain the test error in Table 10.1, we first write\n",
    "a simple function `accuracy()` that compares predicted and true\n",
    "class labels, and then use it to evaluate our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604caf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy <- function(pred, truth) {\n",
    "   mean(pred == truth) }\n",
    "\n",
    "# gets the true classes from all observations in test_ds.\n",
    "truth <- sapply(seq_along(test_ds), function(x) test_ds[x][[2]])\n",
    "\n",
    "fitted %>% \n",
    "  predict(test_ds) %>% \n",
    "  torch_argmax(dim = 2) %>%  # the predicted class is the one with higher 'logit'.\n",
    "  as_array() %>% # we convert to an R object\n",
    "  accuracy(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9bc84",
   "metadata": {},
   "source": [
    "The table also reports LDA (Chapter 4) and multiclass logistic\n",
    "regression. Although packages such as `glmnet` can handle\n",
    "multiclass logistic regression, they are quite slow on this large\n",
    "dataset. It is much faster and quite easy to fit such a model\n",
    "using the `luz` software. We just have an input layer and output layer, and omit the hidden layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "modellr <- nn_module(\n",
    "  initialize = function() {\n",
    "    self$linear <- nn_linear(784, 10)\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    self$linear(x)\n",
    "  }\n",
    ")\n",
    "print(modellr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14ffc6",
   "metadata": {},
   "source": [
    "We fit the model just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f84af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_modellr <- modellr %>% \n",
    "  setup(\n",
    "    loss = nn_cross_entropy_loss(),\n",
    "    optimizer = optim_rmsprop,\n",
    "    metrics = list(luz_metric_accuracy())\n",
    "  ) %>% \n",
    "  fit(\n",
    "    data = train_ds, \n",
    "    epochs = 5,\n",
    "    valid_data = 0.2,\n",
    "    dataloader_options = list(batch_size = 128)\n",
    "  )\n",
    "\n",
    "fit_modellr %>% \n",
    "  predict(test_ds) %>% \n",
    "  torch_argmax(dim = 2) %>%  # the predicted class is the one with higher 'logit'.\n",
    "  as_array() %>% # we convert to an R object\n",
    "  accuracy(truth)\n",
    "\n",
    "\n",
    "# alternatively one can use the `evaluate` function to get the results\n",
    "# on the test_ds\n",
    "evaluate(fit_modellr, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f15eaa",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "In this section we fit a CNN to the `CIFAR` data, which is available in the `torchvision`\n",
    "package. It is arranged in a similar fashion as the `MNIST` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e413d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform <- function(x) {\n",
    "  transform_to_tensor(x)\n",
    "}\n",
    "\n",
    "train_ds <- cifar100_dataset(\n",
    "  root = \"./\", \n",
    "  train = TRUE, \n",
    "  download = TRUE, \n",
    "  transform = transform\n",
    ")\n",
    "\n",
    "test_ds <- cifar100_dataset(\n",
    "  root = \"./\", \n",
    "  train = FALSE, \n",
    "  transform = transform\n",
    ")\n",
    "\n",
    "str(train_ds[1])\n",
    "length(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b98a1",
   "metadata": {},
   "source": [
    "The CIFAR dataset consists of 50,000 training images, each represented by a 3d tensor:\n",
    "each three-color image is represented as a set of three channels, each of which consists of\n",
    "$32\\times 32$ eight-bit pixels. We standardize as we did for the\n",
    "digits, but keep the array structure. This is accomplished with the `transform` argument.\n",
    "\n",
    "Before we start, we look at some of the training images; similar code produced\n",
    "Figure 10.5 on page 411."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mar = c(0, 0, 0, 0), mfrow = c(5, 5))\n",
    "index <- sample(seq(50000), 25)\n",
    "for (i in index) plot(as.raster(as.array(train_ds[i][[1]]$permute(c(2,3,1)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56506cd",
   "metadata": {},
   "source": [
    "The `as.raster()` function converts the feature map so that it can be plotted as a color image.\n",
    "\n",
    "Here we specify a moderately-sized  CNN for\n",
    "demonstration purposes, similar in structure to Figure 10.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b465d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_block <- nn_module(\n",
    "  initialize = function(in_channels, out_channels) {\n",
    "    self$conv <- nn_conv2d(\n",
    "      in_channels = in_channels, \n",
    "      out_channels = out_channels, \n",
    "      kernel_size = c(3,3), \n",
    "      padding = \"same\"\n",
    "    )\n",
    "    self$relu <- nn_relu()\n",
    "    self$pool <- nn_max_pool2d(kernel_size = c(2,2))\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    x %>% \n",
    "      self$conv() %>% \n",
    "      self$relu() %>% \n",
    "      self$pool()\n",
    "  }\n",
    ")\n",
    "\n",
    "model <- nn_module(\n",
    "  initialize = function() {\n",
    "    self$conv <- nn_sequential(\n",
    "      conv_block(3, 32),\n",
    "      conv_block(32, 64),\n",
    "      conv_block(64, 128),\n",
    "      conv_block(128, 256)\n",
    "    )\n",
    "    self$output <- nn_sequential(\n",
    "      nn_dropout(0.5),\n",
    "      nn_linear(2*2*256, 512),\n",
    "      nn_relu(),\n",
    "      nn_linear(512, 100)\n",
    "    )\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    x %>% \n",
    "      self$conv() %>% \n",
    "      torch_flatten(start_dim = 2) %>% \n",
    "      self$output()\n",
    "  }\n",
    ")\n",
    "model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0332be4",
   "metadata": {},
   "source": [
    "Notice that we used the `padding = \"same\"` argument to\n",
    "`nn_conv2d()`, which ensures that the output channels have the\n",
    "same dimension as the input channels. There are 32 channels in the first\n",
    "hidden layer, in contrast to the three channels in the input layer. We\n",
    "use a $3\\times 3$ convolution filter for each channel in all the layers. Each\n",
    "convolution is followed by a max-pooling layer over $2\\times2$\n",
    "blocks. By studying the summary, we can see that the channels halve in both\n",
    "dimensions\n",
    "after each of these max-pooling operations. After the last of these we\n",
    "have a layer with  256 channels of dimension $2\\times 2$. These are then\n",
    "flattened to a dense layer of size 1,024:\n",
    "in other words, each of the $2\\times 2$ matrices is turned into a\n",
    "$4$-vector, and put side-by-side in one layer. This is followed by a\n",
    "dropout regularization layer,  then\n",
    "another dense layer of size 512, and finally, the\n",
    "output layer.\n",
    "\n",
    "Finally, we specify the fitting algorithm, and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962929fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted <- model %>% \n",
    "  setup(\n",
    "    loss = nn_cross_entropy_loss(),\n",
    "    optimizer = optim_rmsprop, \n",
    "    metrics = list(luz_metric_accuracy())\n",
    "  ) %>% \n",
    "  set_opt_hparams(lr = 0.001) %>% \n",
    "  fit(\n",
    "    train_ds,\n",
    "    epochs = 10, #30,\n",
    "    valid_data = 0.2,\n",
    "    dataloader_options = list(batch_size = 128)\n",
    "  )\n",
    "\n",
    "print(fitted)\n",
    "\n",
    "evaluate(fitted, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb275ad",
   "metadata": {},
   "source": [
    "This model takes 10 minutes to run and achieves 36% accuracy on the test\n",
    "data. Although this is not terrible for 100-class data (a random\n",
    "classifier gets 1% accuracy), searching the web we see results around\n",
    "75%. Typically it takes a lot of architecture carpentry,\n",
    "fiddling with regularization, and time to achieve such results.\n",
    "\n",
    "## Using Pretrained CNN Models\n",
    "\n",
    "We now show how to use a CNN pretrained on the  `imagenet` database to classify natural\n",
    "images, and demonstrate how we produced Figure 10.10.\n",
    "We copied six jpeg images from a digital photo album into the\n",
    "directory `book_images`. (These images are available\n",
    "  from the data section of  [www.statlearning.com](www.statlearning.com), the ISLR book website. Download `book_images.zip`; when\n",
    "clicked it creates the `book_images` directory.) We first read in the images, and\n",
    "convert them into the array format expected by the `torch`\n",
    "software to match the specifications in `imagenet`. Make sure that your working directory in `R` is set to the folder in which the images are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir <- \"book_images\"\n",
    "image_names <- list.files(img_dir)\n",
    "num_images <- length(image_names)\n",
    "x <- torch_empty(num_images, 3, 224, 224)\n",
    "for (i in 1:num_images) {\n",
    "   img_path <- file.path(img_dir, image_names[i])\n",
    "   img <- img_path %>% \n",
    "     base_loader() %>% \n",
    "     transform_to_tensor() %>% \n",
    "     transform_resize(c(224, 224)) %>% \n",
    "     # normalize with imagenet mean and stds.\n",
    "     transform_normalize(\n",
    "       mean = c(0.485, 0.456, 0.406),\n",
    "       std = c(0.229, 0.224, 0.225)\n",
    "     )\n",
    "   x[i,,, ] <- img\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ffe29",
   "metadata": {},
   "source": [
    "We then load the trained network. The model has 18 layers, with a fair bit of complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b217fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- torchvision::model_resnet18(pretrained = TRUE)\n",
    "model$eval() # put the model in evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c95fac",
   "metadata": {},
   "source": [
    "Finally, we classify our six images, and return the top three class\n",
    "choices in terms of predicted probability for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78155912",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds <- model(x)\n",
    "\n",
    "mapping <- jsonlite::read_json(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\") %>% \n",
    "  sapply(function(x) x[[2]])\n",
    "\n",
    "top3 <- torch_topk(preds, dim = 2, k = 3)\n",
    "\n",
    "top3_prob <- top3[[1]] %>% \n",
    "  nnf_softmax(dim = 2) %>% \n",
    "  torch_unbind() %>% \n",
    "  lapply(as.numeric)\n",
    "\n",
    "top3_class <- top3[[2]] %>% \n",
    "  torch_unbind() %>% \n",
    "  lapply(function(x) mapping[as.integer(x)])\n",
    "\n",
    "result <- purrr::map2(top3_prob, top3_class, function(pr, cl) {\n",
    "  names(pr) <- cl\n",
    "  pr\n",
    "})\n",
    "names(result) <- image_names\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084a758",
   "metadata": {},
   "source": [
    "## IMDb Document Classification\n",
    "\n",
    "Now we perform document classification (Section 10.4) on the `IMDB` dataset, which is available as part of the `torchdatasets`\n",
    "package. We  limit the dictionary size to the\n",
    "10,000  most frequently-used words and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babceaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "max_features <- 10000\n",
    "imdb_train <- imdb_dataset(\n",
    "  root = \".\", \n",
    "  download = TRUE,\n",
    "  split=\"train\",\n",
    "  num_words = max_features\n",
    ")\n",
    "imdb_test <- imdb_dataset(\n",
    "  root = \".\", \n",
    "  download = TRUE,\n",
    "  split=\"test\",\n",
    "  num_words = max_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831e2df",
   "metadata": {},
   "source": [
    "Each element of `imdb_train` is a vector of numbers between 1 and\n",
    "10000 (the document), referring to the words found in the dictionary.\n",
    "For example, the first training document is the positive review on\n",
    "page  419. The indices of the first 12 words are given below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train[1]$x[1:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4f81b",
   "metadata": {},
   "source": [
    "To see the words, we create a function, `decode_review()`, that provides a simple interface to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index <- imdb_train$vocabulary\n",
    "decode_review <- function(text, word_index) {\n",
    "   word <- names(word_index)\n",
    "   idx <- unlist(word_index, use.names = FALSE)\n",
    "   word <- c(\"<PAD>\", \"<START>\", \"<UNK>\", word)\n",
    "   words <- word[text]\n",
    "   paste(words, collapse = \" \")\n",
    "}\n",
    "decode_review(imdb_train[1]$x[1:12], word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521285e",
   "metadata": {},
   "source": [
    "Next we write a function to *one-hot* encode each document in a list\n",
    "of documents, and return a binary matrix in sparse-matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49231533",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(Matrix)\n",
    "one_hot <- function(sequences, dimension) {\n",
    "   seqlen <- sapply(sequences, length)\n",
    "   n <- length(seqlen)\n",
    "   rowind <- rep(1:n, seqlen)\n",
    "   colind <- unlist(sequences)\n",
    "   sparseMatrix(i = rowind, j = colind,\n",
    "      dims = c(n, dimension))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c341b2",
   "metadata": {},
   "source": [
    "To construct the sparse matrix, one supplies just the entries that are\n",
    "nonzero. In the last line we call the function `sparseMatrix()` and supply the\n",
    "row indices corresponding to each document and the column indices\n",
    "corresponding to the words in each document, since we omit the\n",
    "values they are taken to be all ones.\n",
    "Words that appear more than once in any given document still get\n",
    "recorded as a one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all values into a list\n",
    "train <- seq_along(imdb_train) %>% \n",
    "  lapply(function(i) imdb_train[i]) %>% \n",
    "  purrr::transpose()\n",
    "test <- seq_along(imdb_test) %>% \n",
    "  lapply(function(i) imdb_test[i]) %>% \n",
    "  purrr::transpose()\n",
    "\n",
    "# num_words + padding + start + oov token = 10000 + 3\n",
    "x_train_1h <- one_hot(train$x, 10000 + 3)\n",
    "x_test_1h <- one_hot(test$x, 10000 + 3)\n",
    "dim(x_train_1h)\n",
    "nnzero(x_train_1h) / (25000 * (10000 + 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf04e67",
   "metadata": {},
   "source": [
    "Only 1.3% of the entries are nonzero, so this amounts to considerable\n",
    "savings in memory.\n",
    "We create a validation set of size 2,000, leaving 23,000 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35788a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3)\n",
    "ival <- sample(seq(along = train$y), 2000)\n",
    "itrain <- seq_along(train$y)[-ival]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df263a3",
   "metadata": {},
   "source": [
    "First we fit a lasso logistic regression model using `glmnet()`\n",
    "on the training data, and evaluate its performance on the validation\n",
    "data. Finally, we plot the accuracy, `acclmv`, as a function of\n",
    "the shrinkage parameter, $\\lambda$. Similar expressions compute the\n",
    "performance on the test data, and were used to produce the left plot\n",
    "in Figure 10.11.\n",
    "\n",
    "The code  takes advantage of the sparse-matrix format of\n",
    "`x_train_1h`, and runs in about 5 seconds; in the usual\n",
    "dense format it would take about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(glmnet)\n",
    "y_train <- unlist(train$y)\n",
    "\n",
    "fitlm <- glmnet(x_train_1h[itrain, ], unlist(y_train[itrain]),\n",
    "    family = \"binomial\", standardize = FALSE)\n",
    "classlmv <- predict(fitlm, x_train_1h[ival, ]) > 0\n",
    "acclmv <- apply(classlmv, 2, accuracy,  unlist(y_train[ival]) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571932a9",
   "metadata": {},
   "source": [
    "We applied the `accuracy()` function that we wrote in Lab 10.9.2\n",
    "to every column of the prediction matrix\n",
    "`classlmv`, and since this is a logical matrix of  `TRUE/FALSE` values, we\n",
    "supply the second argument `truth` as a logical vector as well.\n",
    "\n",
    "Before making a plot, we adjust the plotting window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mar = c(4, 4, 4, 4), mfrow = c(1, 1))\n",
    "plot(-log(fitlm$lambda), acclmv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d4742",
   "metadata": {},
   "source": [
    "Next we fit a fully-connected neural network with two hidden layers,\n",
    "each with 16 units and ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- nn_module(\n",
    "  initialize = function(input_size = 10000 + 3) {\n",
    "    self$dense1 <- nn_linear(input_size, 16)\n",
    "    self$relu <- nn_relu()\n",
    "    self$dense2 <- nn_linear(16, 16)\n",
    "    self$output <- nn_linear(16, 1)\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    x %>% \n",
    "      self$dense1() %>% \n",
    "      self$relu() %>% \n",
    "      self$dense2() %>% \n",
    "      self$relu() %>% \n",
    "      self$output() %>% \n",
    "      torch_flatten(start_dim = 1)\n",
    "  }\n",
    ")\n",
    "model <- model %>% \n",
    "  setup(\n",
    "    loss = nn_bce_with_logits_loss(),\n",
    "    optimizer = optim_rmsprop,\n",
    "    metrics = list(luz_metric_binary_accuracy_with_logits())\n",
    "  ) %>% \n",
    "  set_opt_hparams(lr = 0.001)\n",
    "\n",
    "fitted <- model %>% \n",
    "  fit(\n",
    "    # we transform the training and validation data into torch tensors\n",
    "    list(\n",
    "      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()), \n",
    "      torch_tensor(unlist(train$y[itrain]))\n",
    "    ),\n",
    "    valid_data = list(\n",
    "      torch_tensor(as.matrix(x_train_1h[ival, ]), dtype = torch_float()), \n",
    "      torch_tensor(unlist(train$y[ival]))\n",
    "    ),\n",
    "    dataloader_options = list(batch_size = 512),\n",
    "    epochs = 10\n",
    "  )\n",
    "\n",
    "plot(fitted)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bfc89",
   "metadata": {},
   "source": [
    "The `fitted` object has a `get_metrics` method that\n",
    "gets both the training and validation accuracy at each epoch.\n",
    "Figure 10.11 includes test accuracy at each epoch as well. To\n",
    "compute the test accuracy, we\n",
    "rerun the entire sequence above, replacing the last line with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted <- model %>% \n",
    "  fit(\n",
    "    list(\n",
    "      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()), \n",
    "      torch_tensor(unlist(train$y[itrain]))\n",
    "    ),\n",
    "    valid_data = list(\n",
    "      torch_tensor(as.matrix(x_test_1h), dtype = torch_float()), \n",
    "      torch_tensor(unlist(test$y))\n",
    "    ),\n",
    "    dataloader_options = list(batch_size = 512),\n",
    "    epochs = 10\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ad2f8",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "In this lab we fit the models illustrated in\n",
    "Section 10.5.\n",
    "\n",
    "### Sequential Models for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc7a79a",
   "metadata": {},
   "source": [
    "Here we  fit a simple  LSTM RNN for sentiment analysis with\n",
    "the `IMDB` movie-review data, as discussed in Section 10.5.1. We showed how to input the data in\n",
    "10.9.5, so we will not repeat that here.\n",
    "\n",
    "We first calculate the lengths of the documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc <- sapply(seq_along(imdb_train), function(i) length(imdb_train[i]$x))\n",
    "median(wc)\n",
    "sum(wc <= 500) / length(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820de189",
   "metadata": {},
   "source": [
    "We see that over 91% of the documents have fewer than 500 words. Our\n",
    "RNN requires all the document sequences to have the same length. We hence\n",
    "restrict the document lengths to the last $L=500$ words, and pad the\n",
    "beginning of the\n",
    "shorter ones with blanks. We will use `torchdatasets` functionality for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0a40e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "maxlen <- 500\n",
    "num_words <- 10000\n",
    "imdb_train <- imdb_dataset(root = \".\", split = \"train\", num_words = num_words,\n",
    "                           maxlen = maxlen)\n",
    "imdb_test <- imdb_dataset(root = \".\", split = \"test\", num_words = num_words,\n",
    "                           maxlen = maxlen)\n",
    "\n",
    "vocab <- c(rep(NA, imdb_train$index_from - 1), imdb_train$get_vocabulary())\n",
    "tail(names(vocab)[imdb_train[1]$x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd4547",
   "metadata": {},
   "source": [
    "The last expression shows the last few words in the first document. At this stage, each of the 500 words in the document  is represented using an integer\n",
    "corresponding to the location of that word in the 10,000-word dictionary.\n",
    "The first layer of the RNN is an embedding layer of size 32, which will be\n",
    "learned during  training. This layer one-hot encodes  each document\n",
    "as a matrix of dimension $500 \\times 10,000$, and then maps these\n",
    "$10,000$ dimensions down to $32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d68f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- nn_module(\n",
    "  initialize = function() {\n",
    "    self$embedding <- nn_embedding(10000 + 3, 32)\n",
    "    self$lstm <- nn_lstm(input_size = 32, hidden_size = 32, batch_first = TRUE)\n",
    "    self$dense <- nn_linear(32, 1)\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    c(output, c(hn, cn)) %<-% (x %>% \n",
    "      self$embedding() %>% \n",
    "      self$lstm())\n",
    "    output[,-1,] %>%  # get the last output\n",
    "      self$dense() %>% \n",
    "      torch_flatten(start_dim = 1)\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a777eb",
   "metadata": {},
   "source": [
    "The second  layer is an LSTM with 32 units, and the output\n",
    "layer is a single logit for the binary classification task.\n",
    "The rest is now similar to other networks we have fit. We\n",
    "track the test performance as the network is fit, and see that it attains 87\\% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- model %>% \n",
    "  setup(\n",
    "    loss = nn_bce_with_logits_loss(),\n",
    "    optimizer = optim_rmsprop,\n",
    "    metrics = list(luz_metric_binary_accuracy_with_logits())\n",
    "  ) %>% \n",
    "  set_opt_hparams(lr = 0.001)\n",
    "\n",
    "fitted <- model %>% fit(\n",
    "  imdb_train, \n",
    "  epochs = 10,\n",
    "  dataloader_options = list(batch_size = 128),\n",
    "  valid_data = imdb_test\n",
    ")\n",
    "plot(fitted)\n",
    "predy <- torch_sigmoid(predict(fitted, imdb_test)) > 0.5\n",
    "evaluate(fitted, imdb_test, dataloader_options = list(batch_size = 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a469a",
   "metadata": {},
   "source": [
    "###  Time Series Prediction\n",
    "\n",
    "We now show how to fit the models in Section 10.5.2\n",
    "for  time series prediction.\n",
    "We first set up the data, and standardize each of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39729411",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ISLR2)\n",
    "xdata <- data.matrix(\n",
    " NYSE[, c(\"DJ_return\", \"log_volume\",\"log_volatility\")]\n",
    " )\n",
    "istrain <- NYSE[, \"train\"]\n",
    "xdata <- scale(xdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a2e80",
   "metadata": {},
   "source": [
    "The  variable `istrain` contains a `TRUE` for each year that  is in the training set, and a `FALSE` for each year\n",
    " in the test set.\n",
    "\n",
    "We first write functions to create lagged versions of the three time series.  We start with a function that takes as input a data\n",
    "matrix and a lag $L$, and returns a lagged version of the matrix. It\n",
    "simply inserts $L$ rows of `NA` at the top, and truncates the\n",
    "bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lagm <- function(x, k = 1) {\n",
    "   n <- nrow(x)\n",
    "   pad <- matrix(NA, k, ncol(x))\n",
    "   rbind(pad, x[1:(n - k), ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee3e31",
   "metadata": {},
   "source": [
    "We now use this function to create a data frame with all the required\n",
    "lags, as well as the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377447d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "arframe <- data.frame(log_volume = xdata[, \"log_volume\"],\n",
    "   L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),\n",
    "   L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),\n",
    "   L5 = lagm(xdata, 5)\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324830a0",
   "metadata": {},
   "source": [
    "If we look at the first five rows of this frame, we will see some\n",
    "missing values in the lagged variables (due to the construction above). We remove these rows, and adjust `istrain`\n",
    "accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arframe <- arframe[-(1:5), ]\n",
    "istrain <- istrain[-(1:5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a2691",
   "metadata": {},
   "source": [
    "We now fit the linear AR model to the training data using `lm()`, and predict on the\n",
    "test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "arfit <- lm(log_volume ~ ., data = arframe[istrain, ])\n",
    "arpred <- predict(arfit, arframe[!istrain, ])\n",
    "V0 <- var(arframe[!istrain, \"log_volume\"])\n",
    "1 - mean((arpred - arframe[!istrain, \"log_volume\"])^2) / V0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8b44b",
   "metadata": {},
   "source": [
    "The last two lines compute the $R^2$ on the test data, as defined in (3.17).\n",
    "\n",
    "We refit this model, including the factor variable `day_of_week`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arframed <-\n",
    "    data.frame(day = NYSE[-(1:5), \"day_of_week\"], arframe)\n",
    "arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])\n",
    "arpredd <- predict(arfitd, arframed[!istrain, ])\n",
    "1 - mean((arpredd - arframe[!istrain, \"log_volume\"])^2) / V0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5e0c7",
   "metadata": {},
   "source": [
    "To fit the RNN, we need to reshape these data, since it expects a\n",
    "sequence of $L=5$ feature vectors $X=\\{X_\\ell\\}_1^L$ for each observation, as in (10.20) on\n",
    "page  428. These are  lagged versions of the\n",
    "time series going back $L$ time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55542f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- nrow(arframe)\n",
    "xrnn <- data.matrix(arframe[, -1])\n",
    "xrnn <- array(xrnn, c(n, 3, 5))\n",
    "xrnn <- xrnn[,, 5:1]\n",
    "xrnn <- aperm(xrnn, c(1, 3, 2))\n",
    "dim(xrnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ef7cc",
   "metadata": {},
   "source": [
    "We have done this in four steps. The first simply extracts the\n",
    "$n\\times 15$ matrix of lagged versions of the three predictor\n",
    "variables from `arframe`. The second converts this matrix to a\n",
    "$n\\times 3\\times 5$ array. We can do this by simply changing the\n",
    "dimension attribute, since the new array is filled column wise. The\n",
    "third step reverses the order of lagged variables, so that index $1$\n",
    "is furthest back in time, and index $5$ closest. The\n",
    "final step rearranges the coordinates of the array (like a partial\n",
    "transpose) into the format that the RNN module in `torch`\n",
    "expects.\n",
    "\n",
    "Now we are ready to proceed with the RNN, which uses 12 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- nn_module(\n",
    "  initialize = function() {\n",
    "    self$rnn <- nn_rnn(3, 12, batch_first = TRUE)\n",
    "    self$dense <- nn_linear(12, 1)\n",
    "    self$dropout <- nn_dropout(0.2)\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    c(output, ...) %<-% (x %>% \n",
    "      self$rnn())\n",
    "    output[,-1,] %>% \n",
    "      self$dropout() %>% \n",
    "      self$dense() %>% \n",
    "      torch_flatten(start_dim = 1)\n",
    "  }\n",
    ")\n",
    "\n",
    "model <- model %>% \n",
    "  setup(\n",
    "    optimizer = optim_rmsprop,\n",
    "    loss = nn_mse_loss()\n",
    "  ) %>% \n",
    "  set_opt_hparams(lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41aaef",
   "metadata": {},
   "source": [
    "The output layer has a single unit for the response.\n",
    "\n",
    "We  fit the model in a similar fashion to previous networks. We\n",
    "supply the `fit` function with test data as validation data, so that when\n",
    "we monitor its progress and plot the history function we can see the\n",
    "progress on the test data. Of course we should not use this as a basis for\n",
    "early stopping, since then the test performance would be biased.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted <- model %>% fit(\n",
    "    list(xrnn[istrain,, ], arframe[istrain, \"log_volume\"]),\n",
    "    epochs = 30, # = 200,\n",
    "    dataloader_options = list(batch_size = 64),\n",
    "    valid_data =\n",
    "      list(xrnn[!istrain,, ], arframe[!istrain, \"log_volume\"])\n",
    "  )\n",
    "kpred <- as.numeric(predict(fitted, xrnn[!istrain,, ]))\n",
    "1 - mean((kpred - arframe[!istrain, \"log_volume\"])^2) / V0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe1f4f",
   "metadata": {},
   "source": [
    "This model takes about one minute to train.\n",
    "\n",
    "We could replace the  `nn_module()`  command above with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b53360",
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- nn_module(\n",
    "  initialize = function() {\n",
    "    self$dense <- nn_linear(15, 1)\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    x %>% \n",
    "      torch_flatten(start_dim = 2) %>% \n",
    "      self$dense()\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab85d4",
   "metadata": {},
   "source": [
    "Here, `torch_flatten()` simply takes the input sequence and\n",
    "turns it into a long vector of predictors. This results in a linear AR model.\n",
    "To fit a nonlinear AR model, we could add in a hidden layer.\n",
    "\n",
    "However, since we already have the matrix of lagged variables from the AR\n",
    "model that we fit earlier using the `lm()` command, we can actually fit a nonlinear AR model without needing to perform flattening.\n",
    "We extract the model matrix `x` from `arframed`, which\n",
    "includes the `day_of_week` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24411765",
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- model.matrix(log_volume ~ . - 1, data = arframed)\n",
    "colnames(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ac720",
   "metadata": {},
   "source": [
    "The `-1` in the formula avoids the creation of a column of ones for\n",
    "the intercept. The variable `day\\_of\\_week` is a five-level\n",
    "factor (there are five trading days), and the\n",
    " `-1` results in  five rather than four dummy variables.\n",
    "\n",
    "The rest of the steps to fit a nonlinear AR model should by now be familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arnnd <- nn_module(\n",
    "  initialize = function() {\n",
    "    self$dense <- nn_linear(20, 32)\n",
    "    self$dropout <- nn_dropout(0.5)\n",
    "    self$activation <- nn_relu()\n",
    "    self$output <- nn_linear(32, 1)\n",
    "    \n",
    "  },\n",
    "  forward = function(x) {\n",
    "    x %>% \n",
    "      torch_flatten(start_dim = 2) %>% \n",
    "      self$dense() %>% \n",
    "      self$activation() %>% \n",
    "      self$dropout() %>% \n",
    "      self$output() %>% \n",
    "      torch_flatten(start_dim = 1)\n",
    "  }\n",
    ")\n",
    "arnnd <- arnnd %>% \n",
    "  setup(\n",
    "    optimizer = optim_rmsprop,\n",
    "    loss = nn_mse_loss()\n",
    "  ) %>% \n",
    "  set_opt_hparams(lr = 0.001)\n",
    "\n",
    "fitted <- arnnd %>% fit(\n",
    "    list(x[istrain,], arframe[istrain, \"log_volume\"]),\n",
    "    epochs = 30, \n",
    "    dataloader_options = list(batch_size = 64),\n",
    "    valid_data =\n",
    "      list(x[!istrain,], arframe[!istrain, \"log_volume\"])\n",
    "  )\n",
    "plot(fitted)\n",
    "npred <- as.numeric(predict(fitted, x[!istrain, ]))\n",
    "1 - mean((arframe[!istrain, \"log_volume\"] - npred)^2) / V0"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
