{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e6ddbb-52d3-4a02-9b69-100a4181e3b0",
   "metadata": {},
   "source": [
    "# DS106 Machine Learning : K-Means and K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd4bfc-9b15-4b80-87b4-b8b60eb56371",
   "metadata": {},
   "source": [
    "### Table of Contents <a class=\"anchor\" id=\"DS106L7_toc\"></a>\n",
    "\n",
    "* [Table of Contents](#DS106L7_toc)\n",
    "    * [Page 1 - Introduction](#DS106L7_page_1)\n",
    "    * [Page 2 - Clustering](#DS106L7_page_2)\n",
    "    * [Page 3 - K-means Clustering in Python](#DS106L7_page_3)\n",
    "    * [Page 4 - k-Nearest Neighbors](#DS106L7_page_4)\n",
    "    * [Page 5 - Performing k-Nearest Neighbors in Python](#DS106L7_page_5)\n",
    "    * [Page 6 - KNN Analysis](#DS106L7_page_6)\n",
    "    * [Page 7 - Key Terms](#DS106L7_page_7)\n",
    "    * [Page 8 - Lesson 2 Practice Hands-On](#DS106L7_page_8)\n",
    "    * [Page 9 - Lesson 2 Practice Hands-On Solution](#DS106L7_page_9)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a8c65-ed48-4354-a257-e01b0fd733d4",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 1 - Overview of this Module<a class=\"anchor\" id=\"DS106L7_page_1\"></a>\n",
    "\n",
    "[Back to Top](#DS106L7_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca880c8-28a9-41b6-9d73-7b53c6c41405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"480\"\n",
       "            src=\"https://player.vimeo.com/video/244082598\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x7f96d0260d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "# Tutorial Video Name: k-Means and K-Nearest Neighbors\n",
    "VimeoVideo('244082598', width=720, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ad526-95fa-43c4-bc43-1fe807708ed3",
   "metadata": {},
   "source": [
    "\n",
    "The transcript for the above overview video **[is located here](https://repo.exeterlms.com/documents/V2/DataScience/Video-Transcripts/DSO106-ML-L02overview.zip)**.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Now that you have a good understanding of supervised machine learning, you will move into unsupervised machine learning. Techniques included in unsupervised machine learning including things like *k*-means clustering and *k*-nearest neighbors, decision trees, and random forests.  You will start with *k*-means clustering and *k*-nearest neighbors!\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "* Understand the process of clustering\n",
    "* Perform *k*-means clustering in Python\n",
    "* Understand the difference between *k*-means and *k*-nearest neighbors\n",
    "* Perform *k*-nearest neighbors in Python\n",
    "\n",
    "This lesson will culminate in a hands on in which you explore data on car's miles per gallon using *k*-means and *k*-nearest neighbors in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990ac7a-5a2f-4744-af14-1b7e385ea484",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 2 - Clustering<a class=\"anchor\" id=\"DS106L7_page_2\"></a>\n",
    "\n",
    "[Back to Top](#DS106L7_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612429df-e4ef-4d85-8460-5bc8914079c3",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "*Clustering* is a type of unsupervised learning in which similar data are grouped together. This means the clustering algorithm does not have a label for each piece of data but finds similarities within the data itself. Here's a scenario where this would be useful: pretend the company Netflix released the first season of a new comedy show. Over the next week or so, Netflix continues to gather information about who's watched it and rated the show highly. Every user on their platform has an account containing age, gender, city of residence, and so on. Netflix could then run a clustering analysis algorithm on the gathered data that would produce a cluster of the different demographics that like their show. Perhaps the show is popular amongst male Texans between ages 18-35 or female Canadians between ages 45-65. This would be invaluable information for the company's marketing department, as they can advertise on specific platforms rather than in a generic setting.\n",
    "\n",
    "---\n",
    "\n",
    "# k-Means Clustering\n",
    "\n",
    "_k_-means clustering is a popular algorithm used for cluster analysis via unsupervised machine learning. Once the k-means algorithm is given the list of data and the number of expected clusters, it is capable of calculating how similar each piece of data is to each cluster and assigning to the cluster that best classifies the data. However, the _k_-means algorithm doesn't do this in one step. It takes an iterative approach to clustering the data points. Initially, the _k_ number of points are chosen at random. As the nearest points are clustered to one of the _k_ points, the mean, or location, to the point is recalculated. The cycle is then repeated until all data clusters are added to a cluster. The animated GIF below illustrates how with each iteration of applying the _k_-means algorithm, the _k_ points are refined. Notice that the variation is minimal as the data points converge.\n",
    "\n",
    " Note that two dimensions are easy to visualize but, isn't practical for complex clustering where data is 4th dimensional (or higher).\n",
    "\n",
    "![A graph title iteration number ten. The x axis runs from zero to one in increments of zero point one. The y axis runs from zero point one to zero point nine in increments of zero point one. The graph is broken into three sections, with data in each section plotted in a different color.](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif)\n",
    "\n",
    "Here is the process that *k*-means takes: \n",
    "\n",
    "* You choose the number of clusters (*k*)\n",
    "* Randomly assign each point to a cluster\n",
    "* Calculate the center of each cluster by taking the mean vector of every point within that cluster \n",
    "* Reassign each data point to the cluster where the center is closest\n",
    "* Recalculate and reassign until the data “settles,” or the clusters stop changing \n",
    "\n",
    "---\n",
    "\n",
    "## Illustrated Definitions in Clustering\n",
    "\n",
    "---\n",
    "\n",
    "### 1. _k_ random data observations (k=3)\n",
    "\n",
    "In the illustration below, the value of _k_ is 3. The radius around the point, also referred to as the _point-to-cluster-centroid distance_, then expands to encompass the points nearest to itself. \n",
    "\n",
    "![Three circles, one red, one green, and one blue. The red circle is near the top. The green circle is below the red. The blue circle is to the right of the green circle. Three gray squares in a vertical column are below and to the left of the red circle, and above and to the left of the green circle. Three gray squares are above and to the right of the blue circle. A U shape of six gray squares are below and between the green and blue circles.](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/K_Means_Example_Step_1.svg/498px-K_Means_Example_Step_1.svg.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Nearest mean\n",
    "\n",
    "The shaded areas represent the cluster field. These areas are the boundaries that distinguish one cluster from another. Boundaries are initially created and are adjusted as each data point is associated with the mean of the neighboring points. \n",
    "\n",
    "![A square with three sections, each shaded in a different color. In the red section at the top of the square is a red circle and red square. In the green section, which is the largest section, is a green circle and six green squares. In the blue section to the right of the square is a blue circle and five blue squares.](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/K_Means_Example_Step_2.svg/556px-K_Means_Example_Step_2.svg.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Centroid\n",
    "\n",
    "Upon the completion of adding the points and creating primitive boundaries (as shown in the image above), the _centroid_ of each cluster can be calculated. This newly created centroid point represents \"center of mass\" for all the observations in the cluster.\n",
    "\n",
    "![At top, a pink circle with an arrow pointing downward and leftward to a red circle. Below the red circle are two green squares with lines downward and rightward to a green circle. Just above the green circle is a light green circle with an arrow pointing to the green circle. Below the green circle are four green squares, each with a line to the green circle. To the right are two blue squares, with lines that point upward and rightward to a blue circle. Just below and to the right of the blue circle is a light blue circle with an arrow pointing to the blue circle. Just above and to the right of the blue circle are three blue squares, each with a line to the blue circle.](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/K_Means_Example_Step_3.svg/556px-K_Means_Example_Step_3.svg.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Max Convergence\n",
    "\n",
    "Once steps 2 and 3 are executed repeatedly to refine the centroid mean, the boundaries of the cluster assume their accurate shape.\n",
    "\n",
    "![A square with three sections. In the red section, which is the largest, is a red circle and directly below it two red squares. Below this section is a green section, which contains a green circle and four green squares. To the right of this section is a blue section, which contains a blue circle and five blue squares.](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/K_Means_Example_Step_4.svg/556px-K_Means_Example_Step_4.svg.png)\n",
    "\n",
    "The next section of this lesson will explain, step-by-step, how to conduct k-means clustering in Python. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832358fd-55fa-420a-8aa2-46fbf2097986",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 3 - K-means Clustering in Python<a class=\"anchor\" id=\"DS106L7_page_3\"></a>\n",
    "\n",
    "[Back to Top](#DS106L7_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564e3e7-3a7f-4936-b3a3-2e0a9ab78512",
   "metadata": {},
   "source": [
    "# K-means Clustering in Python\n",
    "\n",
    "Are you ready to get this party started?  Time to perform *k*-means clustering in Python!\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>You may want to watch this <a href=\"https://vimeo.com/528490801\"> recorded live workshop </a> that goes over the material on k-means. </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Import Packages\n",
    "\n",
    "Of course the first thing that needs to be done is to import packages: \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Load in Data\n",
    "\n",
    "Next, you'll want to load in data.  For this, you can use the ```seaborn``` built-in dataset, ```iris```.  Use this line of code to load it in: \n",
    "\n",
    "```python\n",
    "iris = sns.load_dataset('iris')\n",
    "```\n",
    "\n",
    "But if seaborn isn't working for you, **[click here](https://repo.exeterlms.com/documents/V2/DataScience/Machine-Learning/Iris.zip)** to download the data.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "Almost there, but the ```KMeans()``` function cannot handle cells that are strings, so you will create a new DataFrame that is the same as the old one, but without the ```species``` column. If there was data you actually wanted to use in a string variable, you could instead re-code that variable numerically, but in this case, you can just drop species using ```drop()```:\n",
    "\n",
    "```python\n",
    "irisTrimmed = iris.drop('species', axis=1)\n",
    "```\n",
    "\n",
    "Excellent! Now the data is in a format you can use. \n",
    "\n",
    "---\n",
    "\n",
    "## Perform k-Means\n",
    "\n",
    "The next step is pretty straight forward.  You will use the function ```KMeans()``` to specify the number of clusters, and then fit it using ```fit()```: \n",
    "\n",
    "```python\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(irisTrimmed)\n",
    "```\n",
    "\n",
    "This is what you will receive in return:\n",
    "\n",
    "```text\n",
    "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
    "    n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
    "    random_state=None, tol=0.0001, verbose=0)\n",
    "```\n",
    "\n",
    "That’s it! You are done.  Get yourself a cookie…  \n",
    "\n",
    "![A stack of three chocolate chip cookies.](Media/106.L4.10.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "## Utilizing k-Means\n",
    "\n",
    "What? You want to actually be able to use the results?  And after your cookie consumption was validated!  Fine.  How about you plot the data? \n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('K Means')\n",
    "plt.scatter(irisTrimmed['petal_length'], irisTrimmed['petal_width'], c=kmeans.labels_, cmap='viridis')\n",
    "```\n",
    "\n",
    "This creates a figure, adds a title, and creates a scatter plot where ```petal_length``` is the x axis and ```petal_width``` is the y.  If you left it at that, however, you would have a lot of dots that are all grey.  Using the argument ```c=kmeans.labels_```` means that the dots will color based on the created clusters, and ```cmap=``` is an argument for the color scheme. Your graph should look something like this:\n",
    "\n",
    "![A graph titled K means. The x axis runs from one to seven in increments of one. The y axis runs from zero point zero to two point five in increments of zero point five. At the bottom left, data is clustered and is in a purple color. Near the center of the graph and then moving up to the right are data plotted mostly in yellow, with only two data points in purple.](Media/106.L4.5.png)\n",
    "\n",
    "Not bad! You can see that the clustering worked pretty well; the data is showing two distinct clusters of data, one highlighted in purple and the other in yellow.  But how can you get the data back in a usable form?  Luckily, ```kmeans``` has methods for that! \n",
    "\n",
    "```python\n",
    "kmeans.labels_\n",
    "```\n",
    "\n",
    "If you just type in the code above, it will return an array specifying which cluster the data was in:\n",
    "\n",
    "```text\n",
    "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "```\n",
    "\n",
    "You may recognize this as the c value that we plotted before.  However, comparing this array to your data is a little awkward, so add this data back into the most recent DataFrame as a new column:\n",
    "\n",
    "```python\n",
    "irisTrimmed['Group'] = kmeans.labels_\n",
    "```\n",
    "\n",
    "Now you can skim through your data and tell at a glance where each point falls. This will be mainly what you will be doing with *k*-means clustering, but you can also find the center point (centroid) of your clusters as well, using ```cluster_centers_```: \n",
    "\n",
    "```python\n",
    "kmeans.cluster_centers_\n",
    "```\n",
    "\n",
    "Which returns this array:\n",
    "\n",
    "```text\n",
    "array([[6.30103093, 2.88659794, 4.95876289, 1.69587629],\n",
    "       [5.00566038, 3.36981132, 1.56037736, 0.29056604]])\n",
    "```\n",
    "\n",
    "You can also find the total distance of every point from its cluster center using ```inertia_```:\n",
    "\n",
    "```python\n",
    "kmeans.inertia_\n",
    "```\n",
    "\n",
    "The inertia value provided is:\n",
    "\n",
    "```text\n",
    "152.34795176035792\n",
    "```\n",
    "\n",
    "However, it is rare that finding the centroid or the distance from each centroid will be required.  Be sure to focus on plotting your data and knowing how to add the results back into your DataFrame, so you can use it.  \n",
    "\n",
    "For real now - would you like a cookie?   \n",
    "\n",
    "![A hand holding five small cookies, each a different color.](Media/106.L4.11.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aade2-59af-4cbc-b5ed-da31e39f266d",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 4 - k-Nearest Neighbors<a class=\"anchor\" id=\"DS106L7_page_4\"></a>\n",
    "\n",
    "[Back to Top](#DS106L7_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2ed6e-98dd-43f7-8761-6ecc9a13af08",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "*k*-nearest neighbors (or \"KNN\") is an important algorithm for unsupervised machine learning. Basically, KNN looks at all the data points around it (the neighbors) and decides whether data should be classified into one type or another based on the closest data points.   \n",
    "\n",
    "KNN prediction first calculates the distance from the unknown point to every point in the dataset, then sorts them from closest to furthest away, and finally predicts the majority label of the *k* closest points.  *k* is the number of nearest points that are being compared to the unknown point, so a *k* of 3 looks at the 3 points with the shortest distance from the unknown data you're trying to predict.  While the distance can be any metric measure, standard *Euclidean distance* is the most common choice. \n",
    "\n",
    "It should be noted that the *k* you choose can impact what category a point will fall into.  The higher the *k*, the more bias you introduce.  Your results will be much cleaner, but you will falsely categorize more datapoints.\n",
    "\n",
    "The number of samples can be a user-defined constant (*k*-nearest neighbor learning), or vary based on the local density of points (*radius-based neighbor learning*). Neighbors-based methods are known as *non-generalizing machine learning* methods, since they simply “remember” all of the training data. \n",
    "\n",
    "---\n",
    "\n",
    "## Example KNN Classification\n",
    "\n",
    "If you look at this image, the green point is the unknown, or the data point you are trying to predict.  You are trying to gauge whether the green circle should fall with the blue squares, or with the red triangles.  If you looked at a *k* of 3 (surrounded by the solid black line) then you may assume that the green circle should really be classified as a red triangle.  But if you were to expand to a *k* of 6 (surrounded by the dotted black line) then you may assume that instead, the green circle should be classified as a blue triangle.  This demonstrates that the number of neighbors is important! Luckily, functions in Python will not only help you run KNN, but also help you determine the optimal *k* for any given dataset. \n",
    "\n",
    "![A solid line circle surrounded by a large dashed line circle. Inside the solid line circle is a green circle in the middle with a green question mark above it. To the right are two red triangles. To the left is a blue square. In the space outside the solid line circle but within the dashed line circle are two blue squares. Outside the dashed line circle are three blue squares near the upper left and three red triangles near the top and top right of the space.](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/500px-KnnClassification.svg.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4ad3c-2fe6-457e-8924-1410e951bebd",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 5 - Performing k-Nearest Neighbors in Python<a class=\"anchor\" id=\"DS106L7_page_5\"></a>\n",
    "\n",
    "[Back to Top](#DS106L7_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc06a2-e702-47a1-942f-cbcbe31221a7",
   "metadata": {},
   "source": [
    "# Performing k-Nearest Neighbors in Python\n",
    "\n",
    "KNN in Python is a relatively simple process with very few parameters and it is easy to use and add more data.  However, there are limitations.  Because the distance between every point is calculated, it takes a fair amount of processing power, so it does not work well with large datasets or datasets that have huge numbers of variables.  \n",
    "\n",
    "In order to predict a categorical variable, you will need continuous data.  In this example, you'll use the built-in ```iris``` dataset from ``seaborn``` to predict a species of iris (categorical) from continuous independent variables such as petal length and width.  \n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>You may want to watch this <a href=\"https://vimeo.com/528059230\"> recorded live workshop </a> that goes over the material on KNN. </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Import Packages\n",
    "\n",
    "First, as always, you have to import your packages! \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Load in Data\n",
    "\n",
    "Now you can load in your data.  You will use the following code to access the ```iris``` dataset:\n",
    "\n",
    "```python\n",
    "iris = sns.load_dataset('iris')\n",
    "```\n",
    "\n",
    "If you don't have access to it, you can also **[click here to download it](https://repo.exeterlms.com/documents/V2/DataScience/Machine-Learning/Iris.zip)**.\n",
    "\n",
    "---\n",
    "\n",
    "## Question Setup\n",
    "\n",
    "With this analysis, you are trying to predict which type of iris (```species```) a flower will be based on the other variables in the dataset - length and width of the various flower parts.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "Now, you have data, but of course there's work to do before you get to the fun part! \n",
    "\n",
    "---\n",
    "\n",
    "### Scaling Your Data\n",
    "\n",
    "KNN is based completely on distance from one point to another, so you need to make sure that all of the data is on the same scale.  If some of the variables were in inches and some of it was in yards, it could do some funky things to your results.  Luckily, ```sklearn``` has a tool just for such a happenstance! \n",
    "\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(iris.drop('species', axis=1))\n",
    "scaledVariables = scaler.transform(iris.drop('species',axis=1))\n",
    "irisScaled = pd.DataFrame(scaledVariables, columns=iris.columns[:-1])\n",
    "```\n",
    "\n",
    "This fits the ```StandardScaler()``` function to your data, except for the column that you are predicting, which is ```species```.  You then transform the data and save it as ```scaledVariables```.  Finally, you will turn it into a dataframe that you can work with called ```irisScaled```, which leaves out the predictor variable of species with the code ```[:-1]```.  \n",
    "\n",
    "The new data now looks like this, because it's all been placed on the same scale:\n",
    "\n",
    "![Four columns and five rows of data. Column headings are sepal underscore length, sepal underscore width, petal underscore length, and petal underscore width. Row zero, negative zero point nine zero zero six eight one, one point zero one nine zero zero four, negative one point three four zero two two seven, negative one point three one five four four four. Row one, negative one point one four three zero one seven, negative zero poitn one three one nine seven nine, negative one point three four zero two two seven, negative one point three one five four four four. Row two, negative one point three eight five three five three, zero point three two eight four one four, negative one point three nine seven zero six four, negative one point three one five four four four. Row three, negative one point five zero six five two one, zero point zero nine eight two one seven, negative one point two eight three three eight nine, negative one point three one five four four four. Row four, negative one point zero two one eight four nine, one point two four nine two zero one, negative one point three four zero two two seven, negative one point three one five four four four.](Media/106.L3.11.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Creating x and y Datasets\n",
    "\n",
    "Now you'll need to subset your x and y data: \n",
    "\n",
    "```python\n",
    "x = irisScaled\n",
    "y = iris['species']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Train Test Split\n",
    "\n",
    "The next step is to do the train/test split. The basis of ```train_test_split()``` is just to separate your data, and you can apply many different machine learning techniques to the data once you’ve used train_test_split.\n",
    "\n",
    "```python\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=101)\n",
    "```\n",
    "\n",
    "Where ```x``` is the dataset you have scaled and are testing (independent variables) and y is the thing you are predicting (dependent variables).  The argument ```random_state=``` does not usually have to be set, but, in this case, it will make your results line up with the example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e3b8a-9224-447f-8cf7-5035cae86421",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 6 - KNN Analysis<a class=\"anchor\" id=\"DS106L7_page_6\"></a>\n",
    "\n",
    "[Back to Top](#DS106L7_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e2098-28ef-4ff5-a155-afe824de4323",
   "metadata": {},
   "source": [
    "# KNN Analysis\n",
    "\n",
    "Now that you've taken care of the setup, it is time to take a trial run and actually perform KNN!\n",
    "\n",
    "```python\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train, y_train)\n",
    "pred = knn.predict(x_test)\n",
    "```\n",
    "\n",
    "Done! Wasn’t that easy?  Simply a matter of utilizing the ```KNeighborsClassifier()``` function, specifying the number of neighbors with the argument ```n_neighbors=```, and then fitting it to your model and predicting! In this case the number of neighbors is one to start with.\n",
    "\n",
    "---\n",
    "\n",
    "## Interpret KNN Predictions\n",
    "\n",
    "You can look at pred by itself, as shown below, but it is hard to understand: \n",
    "\n",
    "```text\n",
    "array(['setosa', 'setosa', 'setosa', 'virginica', 'versicolor',\n",
    "       'virginica', 'virginica', 'versicolor', 'virginica', 'setosa',\n",
    "       'virginica', 'setosa', 'setosa', 'virginica', 'virginica',\n",
    "       'versicolor', 'versicolor', 'versicolor', 'setosa', 'versicolor',\n",
    "       'versicolor', 'setosa', 'versicolor', 'versicolor', 'versicolor',\n",
    "       'versicolor', 'versicolor', 'virginica', 'setosa', 'setosa',\n",
    "       'virginica', 'versicolor', 'virginica', 'versicolor', 'virginica',\n",
    "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
    "       'virginica', 'setosa', 'setosa', 'setosa', 'versicolor',\n",
    "       'versicolor'], dtype=object)\n",
    "```\n",
    "\n",
    "So you can use some other ```sklearn``` tools to make this pretty and usable.  You'll call on the functions ```confusion_matrix()``` and ```classification_report()```. Start with the confusion matrix:\n",
    "\n",
    "```python\n",
    "print(confusion_matrix(y_test, pred))\n",
    "```\n",
    "\n",
    "Here is what is produced: \n",
    "\n",
    "```text\n",
    "[[13  0  0]\n",
    " [ 0 19  1]\n",
    " [ 0  1 11]]\n",
    "```\n",
    "\n",
    "This confusion matrix shows how the predicted data lines up with reality. There are three different species of iris: ```setosa```, ```virginica```, and ```versicolor```.  Even though there's no labels on the above chart,  they go in that order.  Imagine it looking something like this with headers: \n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Species</th>\n",
    "        <th>setosa</th>\n",
    "        <th>virginica</th>\n",
    "        <th>versicolor</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>setosa</td>\n",
    "        <td>13</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>virginica</td>\n",
    "        <td>0</td>\n",
    "        <td>19</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>versicolor</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>11</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "So what this means is that 13 iris plants were correctly classified as setosa.  There were no misclassifications.  19 iris plants were correctly classified as virginica, with one accidentally being misclassified as versicolor.  And lastly, 11 iris plants were correctly classified as versicolor, with one accidentally being classified as virginica.  Not many mistakes here, so it looks like this KNN algorithm is pretty darn accurate! Want verification of that accuracy in numbers, though? Then check out the classification report as well:\n",
    "\n",
    "```python\n",
    "print(classification_report(y_test,pred))\n",
    "```\n",
    "\n",
    "Here is what is produced: \n",
    "\n",
    "```text\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      setosa       1.00      1.00      1.00        13\n",
    "  versicolor       0.95      0.95      0.95        20\n",
    "   virginica       0.92      0.92      0.92        12\n",
    "\n",
    "   micro avg       0.96      0.96      0.96        45\n",
    "   macro avg       0.96      0.96      0.96        45\n",
    "weighted avg       0.96      0.96      0.96        45\n",
    "```\n",
    "\n",
    "You want to focus on the ```precision``` column here. The KNN algorithm was 100% correct about predicting which iris plants will be of the setosa species, was 95% accurate in predicting the versicolor species, and was 92% accurate in predicting the virginica species.  Awesome! You can also look at the ```weighted avg``` row for ```precision```, which gives an overall value of 96%. \n",
    "\n",
    "---\n",
    "\n",
    "## Choose the Best Model\n",
    "\n",
    "What if you wanted to get that accuracy level just a bit higher? You could try using the *Elbow Method*, which is a way to plot error to see which number of neighbors is best.\n",
    "\n",
    "```python\n",
    "errorRate = []\n",
    "for i in range(1,40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(x_train, y_train)\n",
    "    predI = knn.predict(x_test)\n",
    "    errorRate.append(np.mean(predI != y_test))\n",
    "```\n",
    "\n",
    "The first line of this creates an empty list called ```errorRate```.  Then you set up a for loop that will run the test on every *k* between 1 and 40 with the ```range()``` function.  Finally, it adds the mean of the error rate to the empty list.  You can look at the list now, but it is a little hard to understand:\n",
    "\n",
    "```text\n",
    "[0.044444444444444446,\n",
    " 0.044444444444444446,\n",
    " 0.022222222222222223,\n",
    " 0.044444444444444446,\n",
    " 0.022222222222222223,\n",
    " 0.044444444444444446,\n",
    " 0.0,\n",
    " 0.0,\n",
    " 0.0,\n",
    " 0.022222222222222223,\n",
    " 0.0,\n",
    " 0.0,\n",
    " 0.022222222222222223,\n",
    " 0.044444444444444446,\n",
    " 0.044444444444444446,\n",
    " 0.044444444444444446,\n",
    " 0.022222222222222223,\n",
    " 0.044444444444444446,\n",
    " 0.06666666666666667,\n",
    " 0.06666666666666667,\n",
    " 0.06666666666666667,\n",
    " 0.06666666666666667,\n",
    " 0.06666666666666667,\n",
    " 0.06666666666666667,\n",
    " 0.06666666666666667,\n",
    " 0.06666666666666667,\n",
    " 0.06666666666666667,\n",
    " 0.08888888888888889,\n",
    " 0.08888888888888889,\n",
    " 0.1111111111111111,\n",
    " 0.1111111111111111,\n",
    " 0.15555555555555556,\n",
    " 0.15555555555555556,\n",
    " 0.15555555555555556,\n",
    " 0.13333333333333333,\n",
    " 0.15555555555555556,\n",
    " 0.13333333333333333,\n",
    " 0.13333333333333333,\n",
    " 0.1111111111111111]\n",
    "```\n",
    "\n",
    "Which is why you can plot it! Here's how - and you can choose your own labels, colors, and title: \n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40), errorRate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "```\n",
    "\n",
    "Here is the resulting plot:\n",
    "\n",
    "![A graph titled err rate versus k value. The x axis is labeled K and runs from zero to forty in increments of five. The y axis is labeled error rate and runs from zero point zero zero to zero point one six in increments of zero point zero two. Data is plotted on the graph, with dashed lines connecting each piece of data.](Media/106.L3.8.png)\n",
    "\n",
    "In this case, 7, 8, 9, 11, and 12 are all *k* values that are equally low. Almost no error there!\n",
    "\n",
    "---\n",
    "\n",
    "## Run the Final Model\n",
    "\n",
    "The final step is simply to use one of these *k* values in the model:\n",
    "\n",
    "```python\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(x_train, y_train)\n",
    "pred = knn.predict(x_test)\n",
    "```\n",
    "\n",
    "Then you can look at the new confusion matrix again: \n",
    "\n",
    "```text\n",
    "[[13  0  0]\n",
    " [ 0 20  0]\n",
    " [ 0  0 12]]\n",
    "```\n",
    "\n",
    "And at the new classification report again:\n",
    "\n",
    "```text\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      setosa       1.00      1.00      1.00        13\n",
    "  versicolor       1.00      1.00      1.00        20\n",
    "   virginica       1.00      1.00      1.00        12\n",
    "\n",
    "   micro avg       1.00      1.00      1.00        45\n",
    "   macro avg       1.00      1.00      1.00        45\n",
    "weighted avg       1.00      1.00      1.00        45\n",
    "```\n",
    "\n",
    "When you ran the KNN again with *k*=8, you were able to predict the species of iris with 100% accuracy!  Excellent work!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb7d8a-aced-42e5-ac24-32acf8c95473",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 7 - Key Terms<a class=\"anchor\" id=\"DS106L7_page_7\"></a>\n",
    "\n",
    "[Back to Top](#DS106L7_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0debaf-5fae-4035-8e7d-ae62e62456cb",
   "metadata": {},
   "source": [
    "# Key Terms\n",
    "\n",
    "Below is a list and short description of the important keywords learned in this lesson. Please read through and go back and review any concepts you do not fully understand. Great Work!\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Clustering</td>\n",
    "        <td>Unsupervised learning in which similar data are grouped together.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Centroid</td>\n",
    "        <td>The center of each cluster.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>k-Nearest Neighbor (KNN)</td>\n",
    "        <td>An unsupervised method used for classification and regression.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Radius-Based Neighbor Learning</td>\n",
    "        <td>A type of KNN based on the density of points.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Non-Generalizing Machine Learning Method</td>\n",
    "        <td>Remembers all the training data.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key Python Packages\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>sklearn.cluster</td>\n",
    "        <td>Used for k-means analysis.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>sklearn.preprocessing</td>\n",
    "        <td>Contains code for scaling your data.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>sklearn.neighbors</td>\n",
    "        <td>For performing KNN.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>sklearn.metrics</td>\n",
    "        <td>For interpreting KNN.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key Python Code\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>KMeans()</td>\n",
    "        <td>Performs k-means analysis.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>n_clusters=</td>\n",
    "        <td>An argument to KMeans() in which you choose the number of clusters.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>kmeans.labels_</td>\n",
    "        <td>Provides to which cluster data belong.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>kmeans.cluster_centers_</td>\n",
    "        <td>Provides the centroids for each cluster.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>kmeans.inertia_</td>\n",
    "        <td>Provides the distance of every point from its centroid.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>StandardScaler()</td>\n",
    "        <td>Puts all of your data on the same scale for KNN.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>scaler.transform()</td>\n",
    "        <td>Changes the shape of your data after scaling.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>KNeighborsClassifier()</td>\n",
    "        <td>Function for computing KNN.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>n_neighbors=</td>\n",
    "        <td>An argument to KNeighborsClassifier() that allows you to specify the number of clusters.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>confusion_matrix()</td>\n",
    "        <td>Creates a confusion matrix from your KNN.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>classification_report()</td>\n",
    "        <td>Provides information about the accuracy of your model.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3f800-0a14-4d13-8c32-390965c537ca",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 8 - Lesson 2 Practice Hands-On<a class=\"anchor\" id=\"DS106L7_page_8\"></a>\n",
    "\n",
    "[Back to Top](#DS106L7_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597e4b4-ce22-448d-843e-eab5ba74a9c4",
   "metadata": {},
   "source": [
    "\n",
    "This Hands-On will **not** be graded, but you are encouraged to complete it. However, the best way to become a data scientist is to practice.\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Do not submit your project until you have completed all requirements, as you will not be able to resubmit.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## K-Means Hands-On\n",
    "\n",
    "In this Hands-On exercise, you will create a project that will solidify your understanding of *k*-means and *k*-nearest neighbors. This Hands-On will be completed in Python, using your text editor or IDE of choice (e.g. VSCode, Jupyter Notebooks, Spyder, etc.). \n",
    "\n",
    "Determine how cars are grouped together by using the `mpg` dataset built into Seaborn.  Import it using the following code: \n",
    "\n",
    "```python\n",
    "Mpg = sns.load_dataset('mpg')\n",
    "```\n",
    "\n",
    "If seaborn isn't working for you, **[click here](https://repo.exeterlms.com/documents/V2/DataScience/Machine-Learning/Mpg.zip)** to download the data.\n",
    "\n",
    "Remember that you need continuous variables for these analyses, so you'll want to pinpoint columns such as ```mpg```, ```cylinders```, ```displacement```, ```horsepower```, ```weight```, ```acceleration``` or ```model_year``` as variables.  You'll also need to have those continuous variables as integers...so...hint, hint...there may be a little data wrangling involved.\n",
    "\n",
    "Then use first the *k*-means machine learning algorithm and then the *k*-nearest neighbors algorithm to find the most appropriate *k* to examine, and provide the graph as well as add the cluster labels back into your dataframe.  How are these groups being divided? What conclusions can you draw about the data?\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Be sure to zip and submit your entire directory when finished!</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36943bcd-7e16-4af1-97d3-c09e81ae5e99",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 9 - Lesson 2 Practice Hands-On Solution<a class=\"anchor\" id=\"DS106L7_page_9\"></a>\n",
    "\n",
    "[Back to Top](#DS106L7_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2cdde-9447-49b5-8936-17d80f46b45e",
   "metadata": {},
   "source": [
    "# Lesson 2 Practice Hands-On Solution\n",
    "\n",
    "The Jupyter Notebook containing the Lesson 3 Practice Hands-On solution is located **[here](https://repo.exeterlms.com/documents/V2/DataScience/Machine-Learning/DSO106MLL3handson.zip)**. Please make sure to download a copy and then open with your own Juptyer Notebook program."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
