{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e6ddbb-52d3-4a02-9b69-100a4181e3b0",
   "metadata": {},
   "source": [
    "# DS106 Modeling : Modeling with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd4bfc-9b15-4b80-87b4-b8b60eb56371",
   "metadata": {},
   "source": [
    "### Table of Contents <a class=\"anchor\" id=\"DS106L2_toc\"></a>\n",
    "\n",
    "* [Table of Contents](#DS106L2_toc)\n",
    "    * [Page 1 - Introduction](#DS106L2_page_1)\n",
    "    * [Page 2 - What is Logistic Regression?](#DS106L2_page_2)\n",
    "    * [Page 3 - Assumptions of Logistic Regression](#DS106L2_page_3)\n",
    "    * [Page 4 - Logistic Regression Setup in R](#DS106L2_page_4)\n",
    "    * [Page 5 - Running Logistic Regression and Interpreting the Output](#DS106L2_page_5)\n",
    "    * [Page 6 - Logistic Regression in Python](#DS106L2_page_6)\n",
    "    * [Page 7 - Key Terms](#DS106L2_page_7)\n",
    "    * [Page 8 - Lesson 2 Hands-On](#DS106L2_page_8)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a8c65-ed48-4354-a257-e01b0fd733d4",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 1 - Overview of this Module<a class=\"anchor\" id=\"DS106L2_page_1\"></a>\n",
    "\n",
    "[Back to Top](#DS106L2_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f428e90-d5a3-4674-8347-b9e869581e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"480\"\n",
       "            src=\"https://player.vimeo.com/video/246121316\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x7fbfd1259130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "# Tutorial Video Name: Modeling with logistic regression\n",
    "VimeoVideo('246121316', width=720, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ad526-95fa-43c4-bc43-1fe807708ed3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lesson, you will learn how to compute logistic regressions, which have a categorical dependent variable instead of a continuous variable. By the end of the lesson, you should be able to:\n",
    "\n",
    "* Understand the theory behind logistic regression\n",
    "* Recognize the assumptions for logistic regression\n",
    "* Test assumptions and compute logistic regression in R\n",
    "* Compute logistic regression in Python\n",
    "\n",
    "This lesson will culminate in a hands-on in which you will complete logistic regression in R.\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>You may want to watch this <a href=\"https://vimeo.com/465050172\"> recorded live workshop </a> that goes over the concepts in this lesson.</p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990ac7a-5a2f-4744-af14-1b7e385ea484",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 2 - What is Logistic Regression?<a class=\"anchor\" id=\"DS106L2_page_2\"></a>\n",
    "\n",
    "[Back to Top](#DS106L2_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612429df-e4ef-4d85-8460-5bc8914079c3",
   "metadata": {},
   "source": [
    "# What is Logistic Regression?\n",
    "\n",
    "Now that you have covered simple linear regression, where you have a single independent, continuous variable predicting a single dependent, continuous variable, jump into logistic regression! Logistic regression goes by several names:\n",
    "\n",
    "* Logistic regression (of course)\n",
    "* Logit regression\n",
    "* Logit model\n",
    "\n",
    "Logistic regression is a type of regression that works when you have a single categorical dependent variable. For this lesson, you will only consider that the dependent categorical variable can take on two values. If you want to get technical, this would be called *Binary Logistic Regression*.  If you have more than two levels of your dependent variable, then you have *Multiple Logistic Regression*. \n",
    "\n",
    "To give you an example of what a binary dependent variable might look like, you could have: \n",
    "\n",
    "* 0 and 1\n",
    "* Win and Loss\n",
    "* Pass and Fail\n",
    "* Success and Failure\n",
    "* Healthy and Sick\n",
    "* Alive and Dead\n",
    "\n",
    "---\n",
    "\n",
    "## An Example\n",
    "\n",
    "Take a look at an example. The following spreadsheet shows results for 20 students that took an exam. The data collected are the amount of time spent studying for the exam, and whether or not they passed the exam:\n",
    "\n",
    "![A spreadsheet with two columns, hours and outcome. Rows beneath the hours heading the amount of time a student studied for an exam. The rows beneath the outcome heading show fail or pass.](Media/L02-02.png)\n",
    "\n",
    "Do some eyeball analysis on these data for a few minutes. Even though the data set is pretty limited, it seems that studying for 1.5 hours or less is a recipe for failure. On the other hand, everyone who studied 4 hours or more passed the test. It's difficult to say with anyone who studied between 1.5 and 4 hours, though. Without doing much math yet, create a simple model that would help explain the likelihood of passing the exam. Set up a graph to try and model the probability of passing the exam, based on the amount of study. Here is what you know for sure:\n",
    "\n",
    "![A graph showing the probability of passing an exam based on hours of study. The x axis is labeled hours of study and runs from zero to seven in increments of one. The x axis is also labeled fail. A horizontal line at the top of the graph is labeled pass. On the fail line, a thick red line runs from zero to one point five. On the pass line, a thick red line runs from four to seven.](Media/L02-03.png)\n",
    "\n",
    "If the thick red lines represents whether a student passes or fails based on study time, then anything between 1.5 hours and 4 hours is variable. If you stick with the thought that more study improves your chances of passing, you might just simply connect the two horizontal lines, like this:\n",
    "\n",
    "![A graph showing the probability of passing an exam based on hours of study. The x axis is labeled hours of study and runs from zero to seven in increments of one. The x axis is also labeled fail. A horizontal line at the top of the graph is labeled pass. On the fail line, a thick red line runs from zero to one point five. The line then moves upward on the chart to the pass line. On the pass line, a thick red line runs from four to seven.](Media/L02-04.png)\n",
    "\n",
    "Now add a scale to the vertical axis, going from 0 (fail) to 1 (pass):\n",
    "\n",
    "![A graph showing the probability of passing an exam based on hours of study. The x axis is labeled hours of study and runs from zero to seven in increments of one. The x axis is also labeled fail. The y axis runs from zero to one in increments of zero point two five. A horizontal line at the top of the graph is labeled pass. On the fail line, a thick red line runs from zero to one point five. The line then moves upward on the chart to the pass line. On the pass line, a thick red line runs from four to seven.](Media/L02-05.png)\n",
    "\n",
    "Now you can make some statements about probability. For instance, you might ask \"What is the probability of passing, assuming that I study for 3.25 hours?\" To answer this based on your 'non-mathematical' model, you simply draw a vertical line straight up from 3.25 until it intersects with the thick red line. At that point, the line should be drawn horizontally and to the left, until it hits the vertical axis, like the green line added below:\n",
    "\n",
    "![A graph showing the probability of passing an exam based on hours of study. The x axis is labeled hours of study and runs from zero to seven in increments of one. The x axis is also labeled fail. The y axis runs from zero to one in increments of zero point two five. A horizontal line at the top of the graph is labeled pass. On the fail line, a thick red line runs from zero to one point five. The line then moves upward on the chart to the pass line. On the pass line, a thick red line runs from four to seven. A dotted green line runs from left to right, starting on the y axis just below zero point seven five. When it hits the upward diagonal red line, the dotted green line runs directly downward to the x axis, at about three point two five.](Media/L02-06.png)\n",
    "\n",
    "Using eyeball analysis, it looks like the horizontal dashed green line intersects the vertical axis at about 0.7. This becomes your prediction. You can now make the statement that if you study for 3.25 hours, you have a 70% probability of passing the exam.\n",
    "\n",
    "Now add in the actual data points:\n",
    "\n",
    "![A graph showing the probability of passing an exam based on hours of study. The x axis is labeled hours of study and runs from zero to seven in increments of one. The x axis is also labeled fail. The y axis runs from zero to one in increments of zero point two five. A horizontal line at the top of the graph is labeled pass. On the fail line, a thick red line runs from zero to one point five. The line then moves upward on the chart to the pass line. On the pass line, a thick red line runs from four to seven. A dotted green line runs from left to right, starting on the y axis just below zero point seven five. When it hits the upward diagonal red line, the dotted green line runs directly downward to the x axis, at about three point two five. Data points are plotted on the pass line and the fail line.](Media/L02-07.png)\n",
    "\n",
    "Even though this is different from the 'best fit' line you created in the linear regression lesson, can you see how it is sort of a 'best fit' in a sense, given the parameters within which you are working?\n",
    "\n",
    "Okay, one final thing...typically predictions are smooth curves. Again, without doing any math, a smoothed curve for the prediction might look something like this:\n",
    "\n",
    "![A graph showing the probability of passing an exam based on hours of study. The x axis is labeled hours of study and runs from zero to seven in increments of one. The x axis is also labeled fail. The y axis runs from zero to one in increments of zero point two five. A horizontal line at the top of the graph is labeled pass. On the fail line, a line runs from zero to one point five before beginning to smoothly curve upward. The line then moves upward on the chart toward the pass line. Near the pass line, the line begins to curve before it is horizontal and runs from four to seven. A dotted green line runs from left to right, starting on the y axis just below zero point seven five. When it hits the upward slope of the curve, the dotted green line runs directly downward to the x axis, at about three point two five. Data points are plotted on the pass line and the fail line.](Media/L02-08.png)\n",
    "\n",
    "As you can see, the prediction for 3.25 hours of study is pretty much the same.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832358fd-55fa-420a-8aa2-46fbf2097986",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 3 - Assumptions of Logistic Regression<a class=\"anchor\" id=\"DS106L2_page_3\"></a>\n",
    "\n",
    "[Back to Top](#DS106L2_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564e3e7-3a7f-4936-b3a3-2e0a9ab78512",
   "metadata": {},
   "source": [
    "# Assumptions of Logistic Regression\n",
    "\n",
    "There are five assumptions for logistic regression, that, if met, will mean your regression model is as free of error as it can be.  The assumptions for logistic regression are as follows:\n",
    "\n",
    "1. Meets minimum sample size of having at least one case per cell, with no more than 20% of cells having less than five cases. \n",
    "    > What is meant by cells? If you made a 2x2 chart of the actual and predicted outcomes, you should have at least 1 case in each. \n",
    "2. Linearity in the logit\n",
    "3. Absence of multicollinearity\n",
    "4. Absence of outliers \n",
    "5. Independence of errors\n",
    "\n",
    "You will learn about them all in-depth below.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Meets Minimum Sample Size\n",
    "\n",
    "In order to check this, you just need to make sure that you have at least one case each that: \n",
    "\n",
    "* Met the condition and was predicted to meet the condition\n",
    "* Met the condition but was predicted to fail the condition\n",
    "* Failed the condition and was predicted to fail the condition\n",
    "* Failed the condition and was predicted to meet the condition\n",
    "\n",
    "You'll be able to check this in something called a *confusion matrix*.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Linearity in the Logit\n",
    "\n",
    "In logistic regression, the *logit*, also known as the *log-odds*, is the logarithm (log) of the odds of the probability of your outcome.  You need to make sure that the logit is linearly related to the independent variable. This can be done via graph once you have created the logit term.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Absence of Multicollinearity\n",
    "\n",
    "You will test for the absence of multicollinearity for a logistic regression the exact same way you would for a linear regression - just correlate the independent variables, and if they are too highly correlated (.6/.7ish or higher) than you probably have related independent variables and thus the presence of multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Absence of Outliers\n",
    "\n",
    "Again, you'll test the absence of outliers in the same way that you tested them for linear regression.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Independence of Errors \n",
    "\n",
    "The assumption of independent errors just means that your residuals cannot be related to any part of our data.  You can test this out in several ways, but the easiest is to plot the residuals against your index number (number of rows). You can also run plots for autocorrelation, or plot the residuals versus Moran's I. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aade2-59af-4cbc-b5ed-da31e39f266d",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 4 - Logistic Regression Setup in R<a class=\"anchor\" id=\"DS106L2_page_4\"></a>\n",
    "\n",
    "[Back to Top](#DS106L2_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2ed6e-98dd-43f7-8761-6ecc9a13af08",
   "metadata": {},
   "source": [
    "# Logistic Regression Setup in R \n",
    "\n",
    "Now that you understand the basics of logistic regression, and all the assumptions you will need to meet, you will begin the prep work for logistic regression in R.\n",
    "\n",
    "---\n",
    "\n",
    "## Load in Libraries\n",
    "\n",
    "You will need the following libraries to conduct logistic regression in R: \n",
    "\n",
    "```{r}\n",
    "library(\"caret\")\n",
    "library(\"magrittr\")\n",
    "library(\"dplyr\")\n",
    "library(\"tidyr\")\n",
    "library(\"lmtest\")\n",
    "library(\"popbio\")\n",
    "library(\"e1071\")\n",
    "```\n",
    "\n",
    "```caret``` and ```lmtest``` will be used to test assumptions, ```dplyr```, ```tidyr```, and ```magrittr``` are used for data wrangling, and ```popbio``` is used to graph your logistic regression model.\n",
    "\n",
    "---\n",
    "\n",
    "## Load in Data\n",
    "\n",
    "For this example, you will use this **[baseball dataset](https://repo.exeterlms.com/documents/V2/DataScience/Modeling-Optimization/baseball.zip)**. Each baseball season, each team plays 162 regular season games. Since baseball doesn't ever end in a tie, each game has a winner and a loser. There are 30 teams, but each game includes two teams, so there are a total of (162 \\* 30) / 2 total games played, or 2430 total games. \n",
    "\n",
    "Since each of the 2430 games has a winner and a loser, you have a table with 4860 rows of data. It looks like this:\n",
    "\n",
    "![A spreadsheet of baseball data. The column headings are game, date, team, opp, W forward slash L, R, R A, D forward slash N, att, date, team, and H R count. Twenty rows of data are shown.](Media/L02-14.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Question Setup\n",
    "\n",
    "Even though runs can be scored a number of different ways, you can investigate how good a predictor home runs are to determine the winner. It seems logical to assume that the more home runs a team hits in a particular game, the more likely they are to win. So you will do a regression where the predictor (IV) is the number of home runs hit by a team, and the response variable (DV) is whether the team wins or loses. This is a case of a quantitative predictor variable, and a categorical response variable.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "The one thing that absolutely has to be done before you can dive into logistic regression is the recoding of the outcome variable (DV) to zeros and ones.  Currently, your wins and losses variable (```W.L```) has a ```W``` indicating a win and an ```L``` indicating a loss.  In R, that just won't fly - you need this outcome variable to be numeric.  The following code will create a new wins and losses column that will re-code this variable numerically:\n",
    "\n",
    "```{r}\n",
    "baseball$WinsR <- NA\n",
    "baseball$WinsR[baseball$W.L=='W'] <- 1\n",
    "baseball$WinsR[baseball$W.L=='L'] <- 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Testing Assumptions\n",
    "\n",
    "Great! Now you are ready to begin testing logistic regression assumptions!\n",
    "\n",
    "---\n",
    "\n",
    "### Appropriate Sample Size\n",
    "\n",
    "The first thing you need to do to test for appropriate sample size is to create the logistic regression model.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Run the Base Logistic Model\n",
    "\n",
    "Just like with linear regression, you typically need to create a model first before you can ensure that it meets all the assumptions - you just won't use it yet.\n",
    "\n",
    "```{r}\n",
    "mylogit <- glm(WinsR ~ HR.Count, data=baseball, family=\"binomial\")\n",
    "```\n",
    "\n",
    "This code should look somewhat familiar to you, as it stems from the same one as your linear regression.  However, instead of just ```lm()``` you now use ```glm()``` and you need to specify ```family=``` . Here you have chose ```binomial``` because you are doing Binomial Logistic Regression. Place your dependent variable first, the new ```WinsR``` column that you re-coded, and then you will add your independent variable after the tilde.  ```baseball``` is the name of your dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### Predict Wins and Losses\n",
    "\n",
    "With that model created (but not interpreted!), you can make predictions about wins and losses.  To do this, you will use the ```predict()``` function on your logit model first: \n",
    "\n",
    "```{r}\n",
    "probabilities <- predict(mylogit, type = \"response\")\n",
    "```\n",
    "\n",
    "Then convert your probabilities to a positive and negative prediction by having anything above .5 (half) be positive, and anything below .5 be negative. This will be done using the ```ifelse()``` function on the ```probabilities``` variable you just created, and it will be assigned to your baseball data set, as the column ```Predicted```, so that you can later compare it with the recoded wins and losses column.\n",
    "\n",
    "```{r}\n",
    "probabilities <- predict(mylogit, type = \"response\")\n",
    "baseball$Predicted <- ifelse(probabilities > .5, \"pos\", \"neg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Recode the Predicted Variable\n",
    "\n",
    "Just like you recoded wins and losses, you also need to recode your new Predicted variable: \n",
    "\n",
    "```{r}\n",
    "baseball$PredictedR <- NA\n",
    "baseball$PredictedR[baseball$Predicted=='pos'] <- 1\n",
    "baseball$PredictedR[baseball$Predicted=='neg'] <- 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Convert Variables to Factors\n",
    "\n",
    "The next thing you need to do is to convert the ```WinsR``` and the ```PredictedR``` columns to factors.  This is necessary because the next line of code you will run requires these variables to be factors. Simply specify the dataset and call the variable before the arrow, then use the function ```as.factor()``` and call the variable again.  \n",
    "\n",
    "```{r}\n",
    "baseball$PredictedR <- as.factor(baseball$PredictedR)\n",
    "baseball$WinsR <- as.factor(baseball$WinsR)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Create a Confusion Matrix\n",
    "\n",
    "And now you are finally ready to create a 2x2 chart, also known as a *confusion matrix*, which will not only test your sample size out, but will also provide some information on the accuracy of our prediction.  Using the ```caret``` library, you will call the ```confusionMatrix()``` function, specifying that you want to compare your predicted values (```PredictedR```) to your actual data, which is the ```WinsR``` column.\n",
    "\n",
    "```{r}\n",
    "conf_mat <- caret::confusionMatrix(baseball$PredictedR, baseball$WinsR)\n",
    "conf_mat\n",
    "```\n",
    "\n",
    "The results are shown below:\n",
    "\n",
    "```text\n",
    "Confusion Matrix and Statistics\n",
    "\n",
    "          Reference\n",
    "Prediction    0    1\n",
    "         0 1917 1240\n",
    "         1  513 1190\n",
    "                                          \n",
    "               Accuracy : 0.6393          \n",
    "                 95% CI : (0.6256, 0.6528)\n",
    "    No Information Rate : 0.5             \n",
    "    P-Value [Acc > NIR] : < 2.2e-16       \n",
    "                                          \n",
    "                  Kappa : 0.2786          \n",
    " Mcnemar's Test P-Value : < 2.2e-16       \n",
    "                                          \n",
    "            Sensitivity : 0.7889          \n",
    "            Specificity : 0.4897          \n",
    "         Pos Pred Value : 0.6072          \n",
    "         Neg Pred Value : 0.6988          \n",
    "             Prevalence : 0.5000          \n",
    "         Detection Rate : 0.3944          \n",
    "   Detection Prevalence : 0.6496          \n",
    "      Balanced Accuracy : 0.6393          \n",
    "                                          \n",
    "       'Positive' Class : 0              \n",
    "```\n",
    "\n",
    "The first thing you will notice is the table at the top.  This is your 2x2 chart, and it shows the following: \n",
    "\n",
    "* **Top left corner (Reference: 0, Prediction: 0):**  These are the cases that failed the condition and were predicted to fail the condition.  In your case, a loss was predicted and a loss actually happened.  \n",
    "    > This is the number you accurately predicted as \"did not happen.\"\n",
    "* **Top right corner (Reference: 1, Prediction: 0):** These are the cases that were predicted to fail the condition, but did not actually fail.  In terms of the current dataset: a loss was predicted, but the team actually won.\n",
    "* **Bottom left corner (Reference: 0, Prediction: 1):** These are the cases where a success was predicted, but a failure actually happened.  This means that the team was predicted to win, but they actually lost. \n",
    "* **Bottom right corner (Reference: 1, Prediction: 1):** These are the cases were a success was predicted and a success actually happened. So, the team was predicted to win and they actually won. \n",
    "    > This is the number you accurately predicted as \"did happen.\"\n",
    "\n",
    "If any one of these four cells in the chart is below 5, then you do not meet the minimum sample size for binary logistic regression.  Luckily, you have a large dataset, and so you pass this assumption!\n",
    "\n",
    "There is also some other useful information contained within the confusion matrix output, however. The accuracy rate shows how accurate your predictions are.  With a .639 accuracy rate, this means that roughly 64% of the time, your predictions are correct. If you added additional independent variables to your model, then perhaps this accuracy rate would go up. The confusion matrix also has information on sensitivity, specificity, the positive predicted value, and the negative predicted value.  Although those are not statistics you will focus on now, they will come up again later when you discuss receiver operator curve analyses, so it's worth getting a heads up on where they can be located in R.  \n",
    "\n",
    "---\n",
    "\n",
    "### Logit Linearity \n",
    "\n",
    "Now you have your model and your predictions, you can calculate the logit and then graph it against your predicted values.\n",
    "\n",
    "You will need to do a little more data wrangling to properly create your logit. You only want to assess the linearity of the logit with numeric variables, so using the library ```dplyr```, and the ```select_if()``` function, you can select only numeric columns from the full dataset by specifying as the argument ```is.numeric```.\n",
    "\n",
    "```{r}\n",
    "baseball1 <- baseball %>% \n",
    "dplyr::select_if(is.numeric)\n",
    "```\n",
    "\n",
    "Then you will pull the rename the column names to be fed into predictors using the ```colnames()``` function: \n",
    "\n",
    "```{r}\n",
    "predictors <- colnames(baseball1)\n",
    "```\n",
    "\n",
    "And finally you can create the logit, using ```tidyr```'s ```mutate()``` and ```gather()``` functions. The logit is calculated as the log of the probabilities divided by one minus the probabilities.\n",
    "\n",
    "```{r}\n",
    "baseball1 <- baseball1 %>%\n",
    "mutate(logit=log(probabilities/(1-probabilities))) %>%\n",
    "gather(key= \"predictors\", value=\"predictor.value\", -logit)\n",
    "```\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Fun Fact!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>If you haven't encountered %>% before, it basically means that you are stringing things together. It comes from the magrittr library and is often found in use with the dplyr library in particular. But anything that uses %>% can be written out in its long form as well. </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "With this logit in hand, you can graph to assess for linearity, using your dear friend, ```ggplot```!\n",
    "\n",
    "```{r} \n",
    "ggplot(baseball1, aes(logit, predictor.value))+\n",
    "geom_point(size=.5, alpha=.5)+\n",
    "geom_smooth(method= \"loess\")+\n",
    "theme_bw()+\n",
    "facet_wrap(~predictors, scales=\"free_y\")\n",
    "```\n",
    "\n",
    "![Six graphs resulting from the use of the g g plot function. The horizontal axis for each is logit. The vertical axis is predictor dot value. Top row of three graphs, from left to right, Att, Game, H R count. Bottom row of three graphs, from left to right, R, R A, Wins R.](Media/106.L7.1.png)\n",
    "\n",
    "This will automatically give you a graph of the logit with every numeric variable.  Of course, in this case, all you care about is the number of home runs, denoted by the upper right hand corner graph labeled ```HR.Count```. Lucky for you, this shows a nice strong linear relationship, so you are good to move on to testing the next assumption!\n",
    "\n",
    "---\n",
    "\n",
    "### Multicollinearity\n",
    "\n",
    "The next assumption you would normally test for is multicollinearity; you can't have your independent variables too closely related to each other.  However, since you only have one independent variable, you can skip this step.\n",
    "\n",
    "---\n",
    "\n",
    "### Independent Errors\n",
    "\n",
    "You can test for independent error by graphing the residual over your index.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Graphing the Errors\n",
    "\n",
    "You can test this a number of ways. The first way is to graph the errors, and there's a nice, easy line of code for this:\n",
    "\n",
    "```{r}\n",
    "plot(mylogit$residuals)\n",
    "```\n",
    "\n",
    "Where ```mylogit``` is the model you created, and residuals is an automatic output of that model that you can call.\n",
    "\n",
    "Here is the graph this code yields:\n",
    "\n",
    "![A graph with a horizontal axis labeled index and a vertical axis labeled my logit residuals. Data is plotted throughout the graph and is mostly clustered at the top.](Media/106.L7.2.png)\n",
    "\n",
    "You are looking for a pretty even distribution of points all the way across your x axis. You have that, so you have met the assumption of independent errors.\n",
    "\n",
    "---\n",
    "\n",
    "#### Use The Durbin-Watson Test\n",
    "\n",
    "Alternatively, you can use the Durbin-Watson test to see whether you have independence of errors.  You'll use the function ```dwtest()``` out of the ```lmtest``` library:\n",
    "\n",
    "```{r}\n",
    "dwtest(mylogit, alternative=\"two.sided\")\n",
    "```\n",
    "\n",
    "Using the ```alternative=\"two.sided\"``` argument means that you are testing for both positive and negative autocorrelation of errors. \n",
    "\n",
    "This code yields the following output: \n",
    "\n",
    "```text\n",
    "\tDurbin-Watson test\n",
    "\n",
    "data:  mylogit\n",
    "DW = 2.0828, p-value = 0.003875\n",
    "alternative hypothesis: true autocorrelation is not 0\n",
    "\n",
    "```\n",
    "\n",
    "If this test is not statistically significant (> .05), then you are automatically good to go, and you have independent errors. However, if it is significant, you can then look at the actual value of the Durbin-Watson test statistic.  If it is under 1 or greater than 3, then you have violated the assumption of independent errors.  Since your DW value is 2.08, you are in an ok range and have met the assumption of independent errors through testing as well as graphing!\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>If you would like more precision in your interpretation of the Durbin-Watson statistic, check out <a href=\"https://www.statisticshowto.com/durbin-watson-test-coefficient/\"> this webpage</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Screening for Outliers\n",
    "\n",
    "To screen for outliers, you will use the ```influence.measures``` function that you used previously: \n",
    "\n",
    "```{r}\n",
    "infl <- influence.measures(mylogit)\n",
    "summary(infl)\n",
    "```\n",
    "\n",
    "To yield this output: \n",
    "\n",
    "```text\n",
    "Potentially influential observations of\n",
    "\t glm(formula = WinsR ~ HR.Count, family = \"binomial\", data = baseball) :\n",
    "\n",
    "     dfb.1_ dfb.HR.C dffit   cov.r   cook.d hat  \n",
    "16   -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "233  -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "275  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "285  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "309  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "320  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "327  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "334  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "345  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "437  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "498  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "501  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "586  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "671  -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "679  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "684  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "694  -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "698  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "777   0.00   0.01     0.01    1.00_*  0.00   0.00\n",
    "779  -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "788   0.04  -0.06    -0.06_*  1.00    0.01   0.00\n",
    "801  -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "904  -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1040 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1088 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1092 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1115 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1133 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1135 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1157 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1213 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1247 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1251 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1258 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1273 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1277 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1280 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1282 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1326 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1330 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1348 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1370 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1377 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1385 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1467 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1474 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1524  0.00   0.01     0.01    1.00_*  0.00   0.00\n",
    "1539 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1549 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1561  0.00   0.01     0.01    1.00_*  0.00   0.00\n",
    "1577 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1582 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1626 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1636 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1645 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1646 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1667 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1677 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1703 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1707 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1718  0.04  -0.06    -0.06_*  1.00    0.01   0.00\n",
    "1754 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1759 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1783 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1811 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1827 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1835 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1849 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1853 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1860 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1862 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1867 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "1879 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1901 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1914 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "1994 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2004 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2011 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2017 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2023 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2033 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2038 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2043 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2067 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2080 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2106 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2148 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2154 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2175 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2206 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2226 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2254 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2289 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2332  0.04  -0.06    -0.06_*  1.00    0.01   0.00\n",
    "2357  0.00   0.01     0.01    1.00_*  0.00   0.00\n",
    "2390 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2410 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2427 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2496 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2500 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2525 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2538 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2573 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2633 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2643  0.00   0.01     0.01    1.00_*  0.00   0.00\n",
    "2657 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2744 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2764 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2773 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2809 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2814 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2831 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2833 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2890 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2917 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "2920 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2936 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2966 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "2992 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3009 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3039 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3060 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3121 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3153 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "3207 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "3243 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "3275 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "3293 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "3315 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3325 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3353 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "3398 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3400 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3410 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3513 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3540 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "3577 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "3588 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3592 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3593  0.04  -0.07    -0.07_*  1.00    0.01   0.00\n",
    "3615 -0.01   0.01     0.01    1.00_*  0.00   0.00\n",
    "3620 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3625  0.04  -0.06    -0.06_*  1.00    0.01   0.00\n",
    "3629  0.00   0.01     0.01    1.00_*  0.00   0.00\n",
    "3655 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3670 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3705 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3714 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3735 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3741 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3742  0.04  -0.06    -0.06_*  1.00    0.01   0.00\n",
    "3755 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3781  0.00   0.01     0.01    1.00_*  0.00   0.00\n",
    "3795 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3841  0.00   0.01     0.01    1.00_*  0.00   0.00\n",
    "3867 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3903 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3920 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3948 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3950 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "3959 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "4034 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "4056 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "4081 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "4090 -0.01   0.01     0.02    1.00_*  0.00   0.00\n",
    "4093  0.00   0.01     0.01    1.00_*  0.00   0.00\n",
    " [ reached getOption(\"max.print\") -- omitted 26 rows ]\n",
    "```\n",
    "\n",
    "Notice that this is not even all the output! You may want to consider creating your own function that will print only the rows that are suspicious.  Remember that if ```dfb.1_``` or ```dffit``` values are greater than 1, or if ```hat``` is greater than .3 or so, you probably have an outlier than should be examined and possibly removed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4ad3c-2fe6-457e-8924-1410e951bebd",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 5 - Running Logistic Regression and Interpreting the Output<a class=\"anchor\" id=\"DS106L2_page_5\"></a>\n",
    "\n",
    "[Back to Top](#DS106L2_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc06a2-e702-47a1-942f-cbcbe31221a7",
   "metadata": {},
   "source": [
    "# Running Logistic Regression and Interpreting the Output\n",
    "\n",
    "Having passed all your assumptions (HOOAH!), you can now proceed to actually calling your logistic regression model and interpreting the output.\n",
    "\n",
    "All you need to do is ask for the summary: \n",
    "\n",
    "```{r}\n",
    "summary(mylogit)\n",
    "```\n",
    "\n",
    "And this is the output it returns:\n",
    "\n",
    "```text\n",
    "Call:\n",
    "glm(formula = WinsR ~ HR.Count, family = \"binomial\", data = baseball)\n",
    "\n",
    "Deviance Residuals: \n",
    "    Min       1Q   Median       3Q      Max  \n",
    "-2.5366  -1.1171  -0.3553   1.2389   1.5338  \n",
    "\n",
    "Coefficients:\n",
    "            Estimate Std. Error z value Pr(>|z|)    \n",
    "(Intercept) -0.80749    0.04658  -17.34   <2e-16 ***\n",
    "HR.Count     0.66398    0.03044   21.81   <2e-16 ***\n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "(Dispersion parameter for binomial family taken to be 1)\n",
    "\n",
    "    Null deviance: 6737.4  on 4859  degrees of freedom\n",
    "Residual deviance: 6161.4  on 4858  degrees of freedom\n",
    "AIC: 6165.4\n",
    "\n",
    "Number of Fisher Scoring iterations: 4\n",
    "```\n",
    "\n",
    "In this output, the first thing to check is whether your independent variable, the number of home runs, was a significant predictor of the number of wins and losses a team had.  Looking in the ```Coefficients``` table under ```HR.Count```, you see that the *p* value is significant at *p* < .001, which is great news. This means that the number of home runs is a significant predictor of the number of wins and losses a team had. The *z* value given next to *p* is the *Wald Statistic*, and you can think of it similarly to the *t* tests you had for individual predictors in linear regression - it's just that Wald works for categorical variables and *t* tests don't.\n",
    "\n",
    "In the same line, the estimate tells you how much the independent variable influences the dependent.  So, for every one unit increase in home runs, you see that the log odds of winning a game (versus losing) are increased by .66.\n",
    "\n",
    "You also have a number of other components here that tell you about model fit, including the deviance residuals, null and residual deviance, and the AIC.  \n",
    "\n",
    "---\n",
    "\n",
    "## Graphing the Logistic Model\n",
    "\n",
    "Want to plot it? You can use the ```popbio``` library to do so with this code: \n",
    "\n",
    "```{r}\n",
    "logi.hist.plot(baseball$HR.Count,baseball$WinsR, boxp=FALSE, type=\"hist\", col=\"gray\")\n",
    "```\n",
    "\n",
    "It will yield this graphic:\n",
    "\n",
    "![A graph resulting from the popbio function. The left side of the graph is labeled probability. The right side of the graph is labeled frequency. A red line slopes upward from left to right across the graph.](Media/106.L7.5.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e3b8a-9224-447f-8cf7-5035cae86421",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 6 - Logistic Regression in Python<a class=\"anchor\" id=\"DS106L2_page_6\"></a>\n",
    "\n",
    "[Back to Top](#DS106L2_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e2098-28ef-4ff5-a155-afe824de4323",
   "metadata": {},
   "source": [
    "# Logistic Regression in Python\n",
    "\n",
    "Although creating a logistic regression model in Python can be done, it is very difficult to test for any of the logistic-specific assumptions such as sample size, linearity of the logit, or independence of errors.  Therefore, you'll be shown just how to create the model; but beware! Without testing assumptions, you will not be able to definitively say that the results are accurate.  Only use Python for logistic regression when the stakes aren't very high and you just need to get an idea of what will happen.\n",
    "\n",
    "---\n",
    "\n",
    "## Import Packages\n",
    "\n",
    "You will only need two packages in Python in order to complete logistic regression: ```pandas``` for reading in your data, and ```statsmodels``` for running the analysis:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Read in Data\n",
    "\n",
    "Next, you'll need to read in your data.  You'll use the **[same baseball dataset](https://repo.exeterlms.com/documents/V2/DataScience/Modeling-Optimization/baseball.zip)** as you did in R.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "There are two components of data wrangling you will need to tackle in Python.  First, you will need to recode your outcome variable into numeric.  Second, you will need to make each variable its own dataframe.\n",
    "\n",
    "---\n",
    "\n",
    "### Recoding the Outcome Variable\n",
    "\n",
    "The wins and losses column, ```W/L```, will need to be recoded from string to numeric: \n",
    "\n",
    "```python\n",
    "def recode (series):\n",
    "    if series == \"W\":\n",
    "        return 1\n",
    "    if series == \"L\":\n",
    "        return 0\n",
    "baseball['WLr'] = baseball['W/L'].apply(recode)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Make Each Variable a Dataframe\n",
    "\n",
    "Then once that is done, you can make your IV and DV into their own dataframes by subsetting.  For convention's sake, you can call them x and y: \n",
    "\n",
    "```python\n",
    "x = baseball['HR Count']\n",
    "y = baseball['WLr']\n",
    "```\n",
    "\n",
    "And that's all the data wrangling you'll need to do before getting on with the analysis!\n",
    "\n",
    "---\n",
    "\n",
    "## Run the Analysis\n",
    "\n",
    "Now you can run your logistic regression code, using the function ```Logit()``` out of the ```statsmodels``` package.  You'll then fit the results, using ```fit()```, and will lastly print out a summary: \n",
    "\n",
    "```python\n",
    "logit = sm.Logit(y,x)\n",
    "results = logit.fit()\n",
    "print(results.summary2())\n",
    "```\n",
    "\n",
    "Here is the output you will receive:\n",
    "\n",
    "```text\n",
    "Optimization terminated successfully.\n",
    "         Current function value: 0.667079\n",
    "         Iterations 5\n",
    "                         Results: Logit\n",
    "================================================================\n",
    "Model:              Logit            Pseudo R-squared: 0.038    \n",
    "Dependent Variable: WLr              AIC:              6486.0070\n",
    "Date:               2019-08-21 11:32 BIC:              6492.4958\n",
    "No. Observations:   4860             Log-Likelihood:   -3242.0  \n",
    "Df Model:           0                LL-Null:          -3368.7  \n",
    "Df Residuals:       4859             LLR p-value:      nan      \n",
    "Converged:          1.0000           Scale:            1.0000   \n",
    "No. Iterations:     5.0000                                      \n",
    "------------------------------------------------------------------\n",
    "            Coef.    Std.Err.      z      P>|z|    [0.025   0.975]\n",
    "------------------------------------------------------------------\n",
    "HR Count    0.2798     0.0184   15.2304   0.0000   0.2438   0.3158\n",
    "================================================================\n",
    "```\n",
    "\n",
    "As you can see in the ```P>|z|``` column, the number of home runs is still a significant predictor of whether a team won or lost the game, and every one home run that a team gets increases their likelihood of winning the game by 28% (as seen in the ```Coef.``` column).  However, in the grand scheme of things, just looking at the number of home runs alone does not seem to have a huge impact, since by looking at the ```Pseudo R-squared``` value, you can see that home runs only explain just under 4% of the variance in winning and losing baseball games.  *Pseudo R-Squared* is very similar to the Multiple R-Squared in regression in interpretation; however, it is calculated differently because you are using logistic  regression, which is why it retains the moniker \"pseudo.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb7d8a-aced-42e5-ac24-32acf8c95473",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 7 - Key Terms<a class=\"anchor\" id=\"DS106L2_page_7\"></a>\n",
    "\n",
    "[Back to Top](#DS106L2_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0debaf-5fae-4035-8e7d-ae62e62456cb",
   "metadata": {},
   "source": [
    "# Key Terms\n",
    "\n",
    "Below is a list and short description of the important keywords learned in this lesson. Please read through and go back and review any concepts you do not fully understand. Great Work!\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Logistic Regression</td>\n",
    "        <td>AKA logit regression or logit model.  A linear regression in which the dependent variable is categorical.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Logit</td>\n",
    "        <td>AKA log-odds. Take the log of probability of the outcome.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key R Libraries\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>caret</td>\n",
    "        <td>For testing regression assumptions.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>lmtest</td>\n",
    "        <td>For testing regression assumptions.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>popbio</td>\n",
    "        <td>For graphing logistic regression.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key R Code\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>glm()</td>\n",
    "        <td>Creates a logistic regression model.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>family=\"binomial\"</td>\n",
    "        <td>An argument to glm() in which you specify that the outcome is a binary category.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>predict()</td>\n",
    "        <td>Makes prediction about your outcome.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>confusionMatrix()</td>\n",
    "        <td>Creates a confusion matrix to test for the assumption of sample size.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>dwtest()</td>\n",
    "        <td>Conducts a Durbin-Watson test for independence of errors.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>logi.hist.plot()</td>\n",
    "        <td>Creates a graph of your logistic model.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key Python Code\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>sm.Logit()</td>\n",
    "        <td>Creates a logistic regression model.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3f800-0a14-4d13-8c32-390965c537ca",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 8 - Lesson 2 Hands-On<a class=\"anchor\" id=\"DS106L2_page_8\"></a>\n",
    "\n",
    "[Back to Top](#DS106L2_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597e4b4-ce22-448d-843e-eab5ba74a9c4",
   "metadata": {},
   "source": [
    "This Hands-­On **will** be graded, so make sure you complete each part. When you are done, please submit one document with all of your findings for grading.\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Do not submit your project until you have completed all requirements, as you will not be able to resubmit.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Modeling with Logistic Regression Hands-On\n",
    "\n",
    "Geologists have long known that the presence of antimony in a ground sample can be a good indication that there is a gold deposit nearby.  In the **[attached data](https://repo.exeterlms.com/documents/V2/DataScience/Modeling-Optimization/minerals.zip)**, you will find antimony (Sb) levels for 64 different locations, and whether or not gold was found nearby.  The \"gold\" column is coded as 0 for no gold present, and 1 for gold present.\n",
    "\n",
    "Use logistic regression in R to create a prediction model that will give the probability of the presence of gold as a response.  Your complete report should contain the following information:\n",
    "\n",
    "1.  Testing and correction for assumptions if necessary. \n",
    "2.  Interpretation of the results in layman's terms.\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Be sure to zip and submit your entire directory when finished!</p>\n",
    "    </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
